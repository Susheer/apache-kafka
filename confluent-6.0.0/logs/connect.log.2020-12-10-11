[2020-12-10 11:00:05,147] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:00:05,150] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:00:05,155] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:00:15,161] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:00:15,161] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:00:15,164] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:00:25,166] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:00:25,166] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:00:25,170] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:00:35,172] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:00:35,172] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:00:35,176] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:00:45,178] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:00:45,178] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:00:45,181] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:00:55,183] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:00:55,184] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:00:55,215] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 31 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:01:05,216] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:01:05,216] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:01:05,219] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:01:15,220] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:01:15,220] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:01:15,224] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:01:25,226] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:01:25,226] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:01:25,230] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:01:28,716] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
org.apache.kafka.connect.errors.ConnectException: Failed to find any class that implements Connector and which name matches com.mongodb.kafka.connect.MongoSinkConnector, available connectors are: PluginDesc{klass=class io.confluent.connect.mqtt.MqttSinkConnector, name='io.confluent.connect.mqtt.MqttSinkConnector', version='0.0.0.0', encodedVersion=0.0.0.0, type=sink, typeName='sink', location='file:/usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0/'}, PluginDesc{klass=class io.confluent.connect.mqtt.MqttSourceConnector, name='io.confluent.connect.mqtt.MqttSourceConnector', version='0.0.0.0', encodedVersion=0.0.0.0, type=source, typeName='source', location='file:/usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0/'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSinkConnector, name='org.apache.kafka.connect.file.FileStreamSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSourceConnector, name='org.apache.kafka.connect.file.FileStreamSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockConnector, name='org.apache.kafka.connect.tools.MockConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=connector, typeName='connector', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSinkConnector, name='org.apache.kafka.connect.tools.MockSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSourceConnector, name='org.apache.kafka.connect.tools.MockSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.SchemaSourceConnector, name='org.apache.kafka.connect.tools.SchemaSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSinkConnector, name='org.apache.kafka.connect.tools.VerifiableSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSourceConnector, name='org.apache.kafka.connect.tools.VerifiableSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}
	at org.apache.kafka.connect.runtime.isolation.Plugins.connectorClass(Plugins.java:208)
	at org.apache.kafka.connect.runtime.isolation.Plugins.newConnector(Plugins.java:180)
	at org.apache.kafka.connect.runtime.AbstractHerder.getConnector(AbstractHerder.java:572)
	at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:342)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$1(AbstractHerder.java:326)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2020-12-10 11:01:35,232] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:01:35,232] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:01:35,235] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:01:45,237] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:01:45,238] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:01:45,241] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:01:55,243] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:01:55,244] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:01:55,248] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:02:05,249] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:02:05,249] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:02:05,253] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:02:15,254] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:02:15,254] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:02:15,258] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:02:25,259] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:02:25,259] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:02:25,262] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:02:35,264] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:02:35,264] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:02:35,267] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:02:44,396] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
org.apache.kafka.connect.errors.ConnectException: Failed to find any class that implements Connector and which name matches com.mongodb.kafka.connect.MongoSinkConnector, available connectors are: PluginDesc{klass=class io.confluent.connect.mqtt.MqttSinkConnector, name='io.confluent.connect.mqtt.MqttSinkConnector', version='0.0.0.0', encodedVersion=0.0.0.0, type=sink, typeName='sink', location='file:/usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0/'}, PluginDesc{klass=class io.confluent.connect.mqtt.MqttSourceConnector, name='io.confluent.connect.mqtt.MqttSourceConnector', version='0.0.0.0', encodedVersion=0.0.0.0, type=source, typeName='source', location='file:/usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0/'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSinkConnector, name='org.apache.kafka.connect.file.FileStreamSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSourceConnector, name='org.apache.kafka.connect.file.FileStreamSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockConnector, name='org.apache.kafka.connect.tools.MockConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=connector, typeName='connector', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSinkConnector, name='org.apache.kafka.connect.tools.MockSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSourceConnector, name='org.apache.kafka.connect.tools.MockSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.SchemaSourceConnector, name='org.apache.kafka.connect.tools.SchemaSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSinkConnector, name='org.apache.kafka.connect.tools.VerifiableSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSourceConnector, name='org.apache.kafka.connect.tools.VerifiableSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}
	at org.apache.kafka.connect.runtime.isolation.Plugins.connectorClass(Plugins.java:208)
	at org.apache.kafka.connect.runtime.isolation.Plugins.newConnector(Plugins.java:180)
	at org.apache.kafka.connect.runtime.AbstractHerder.getConnector(AbstractHerder.java:572)
	at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:342)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$1(AbstractHerder.java:326)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2020-12-10 11:02:45,268] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:02:45,269] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:02:45,274] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:02:55,275] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:02:55,275] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:02:55,278] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:03:05,279] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:03:05,279] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:03:05,283] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:03:15,284] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:03:15,284] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:03:15,287] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:03:25,289] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:03:25,289] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:03:25,294] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:03:35,295] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:03:35,295] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:03:35,311] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 16 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:03:45,313] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:03:45,313] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:03:45,319] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:03:55,319] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:03:55,320] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:03:55,326] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:04:05,327] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:04:05,327] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:04:05,330] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:04:15,331] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:04:15,331] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:04:15,342] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 11 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:04:25,342] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:04:25,343] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:04:25,347] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:04:35,347] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:04:35,348] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:04:35,353] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:04:45,354] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:04:45,354] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:04:45,358] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:04:55,359] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:04:55,359] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:04:55,362] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:05:05,364] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:05:05,364] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:05:05,368] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:05:15,368] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:05:15,369] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:05:15,373] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:05:25,374] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:05:25,374] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:05:25,378] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:05:35,379] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:05:35,379] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:05:35,383] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:05:45,384] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:05:45,385] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:05:45,388] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:05:55,389] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:05:55,389] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:05:55,393] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:06:05,393] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:06:05,393] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:06:05,397] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:06:15,398] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:06:15,398] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:06:15,402] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:06:25,403] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:06:25,403] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:06:25,407] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:06:35,408] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:06:35,408] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:06:35,412] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:06:45,412] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:06:45,412] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:06:45,415] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:06:55,415] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:06:55,416] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:06:55,419] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:07:05,420] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:07:05,420] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:07:05,425] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:07:15,425] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:07:15,426] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:07:15,429] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:07:25,430] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:07:25,430] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:07:25,433] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:07:35,435] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:07:35,435] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:07:35,440] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:07:45,440] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:07:45,440] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:07:45,444] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:07:55,445] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:07:55,445] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:07:55,448] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:08:05,449] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:08:05,449] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:08:05,453] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:08:15,453] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:08:15,453] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:08:15,458] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:08:25,458] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:08:25,458] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:08:25,462] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:08:35,462] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:08:35,463] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:08:35,465] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:08:45,466] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:08:45,467] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:08:45,469] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:08:55,470] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:08:55,470] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:08:55,473] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:09:05,474] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:09:05,474] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:09:05,477] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:09:15,478] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:09:15,478] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:09:15,482] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:09:25,483] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:09:25,483] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:09:25,486] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:09:35,487] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:09:35,487] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:09:35,491] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:09:45,492] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:09:45,493] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:09:45,498] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:09:55,498] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:09:55,498] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:09:55,502] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:10:05,503] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:10:05,503] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:10:05,506] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:10:15,507] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:10:15,507] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:10:15,510] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:10:25,510] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:10:25,511] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:10:25,514] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:10:35,515] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:10:35,515] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:10:35,519] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:10:45,520] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:10:45,520] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:10:45,524] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:10:55,525] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:10:55,525] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:10:55,529] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:11:05,529] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:11:05,530] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:11:05,533] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:11:15,534] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:11:15,534] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:11:15,537] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:11:25,538] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:11:25,538] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:11:25,541] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:11:35,542] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:11:35,542] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:11:35,545] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:11:45,546] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:11:45,546] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:11:45,550] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:11:55,551] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:11:55,551] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:11:55,557] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:12:05,558] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:12:05,558] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:12:05,562] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:12:15,563] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:12:15,564] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:12:15,568] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:12:25,568] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:12:25,568] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:12:25,573] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:12:35,574] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:12:35,574] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:12:35,577] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:12:45,578] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:12:45,579] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:12:45,583] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:12:55,585] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:12:55,585] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:12:55,588] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:13:05,589] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:13:05,589] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:13:05,593] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:13:15,593] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:13:15,594] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:13:15,597] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:13:25,598] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:13:25,599] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:13:25,604] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:13:35,605] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:13:35,605] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:13:35,609] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:13:45,610] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:13:45,610] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:13:45,613] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:13:55,614] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:13:55,614] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:13:55,617] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:14:05,619] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:14:05,620] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:14:05,623] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:14:15,623] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:14:15,623] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:14:15,626] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:14:25,627] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:14:25,627] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:14:25,631] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:14:35,632] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:14:35,632] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:14:35,635] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:14:45,636] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:14:45,636] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:14:45,647] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 11 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:14:55,650] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:14:55,650] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:14:55,653] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:15:05,654] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:15:05,654] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:15:05,659] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:15:15,660] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:15:15,660] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:15:15,667] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:15:25,668] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:15:25,669] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:15:25,672] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:15:35,673] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:15:35,673] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:15:35,677] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:15:45,678] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:15:45,678] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:15:45,681] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:15:55,682] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:15:55,683] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:15:55,686] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:16:05,687] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:16:05,687] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:16:05,690] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:16:15,691] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:16:15,691] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:16:15,694] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:16:25,695] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:16:25,695] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:16:25,698] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:16:35,698] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:16:35,699] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:16:35,702] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:16:45,703] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:16:45,703] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:16:45,707] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:16:55,708] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:16:55,708] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:16:55,713] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:17:05,714] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:17:05,714] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:17:05,718] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:17:15,721] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:17:15,721] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:17:15,726] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:17:25,727] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:17:25,727] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:17:25,730] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:17:35,731] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:17:35,732] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:17:35,735] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:17:45,738] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:17:45,739] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:17:45,742] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:17:55,744] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:17:55,744] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:17:55,747] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:18:05,748] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:18:05,748] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:18:05,751] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:18:15,753] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:18:15,753] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:18:15,756] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:18:25,758] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:18:25,758] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:18:25,762] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:18:35,763] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:18:35,764] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:18:35,767] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:18:45,768] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:18:45,769] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:18:45,772] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:18:55,773] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:18:55,773] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:18:55,776] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:19:05,776] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:19:05,777] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:19:05,781] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:19:15,782] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:19:15,782] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:19:15,786] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:19:25,787] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:19:25,787] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:19:25,790] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:19:35,792] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:19:35,792] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:19:35,795] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:19:45,796] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:19:45,796] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:19:45,800] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:19:55,800] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:19:55,800] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:19:55,804] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:20:05,805] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:20:05,805] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:20:05,808] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:20:15,809] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:20:15,809] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:20:15,812] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:20:25,814] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:20:25,815] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:20:25,818] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:20:35,820] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:20:35,820] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:20:35,823] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:20:45,824] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:20:45,824] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:20:45,827] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:20:55,828] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:20:55,828] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:20:55,831] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:21:05,832] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:21:05,832] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:21:05,835] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:21:15,836] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:21:15,837] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:21:15,852] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 15 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:21:25,854] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:21:25,854] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:21:25,857] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:21:35,858] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:21:35,858] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:21:35,861] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:21:45,863] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:21:45,863] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:21:45,869] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:21:55,871] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:21:55,872] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:21:55,875] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:22:05,876] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:22:05,876] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:22:05,880] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:22:15,881] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:22:15,881] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:22:15,885] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:22:25,885] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:22:25,886] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:22:25,888] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:22:35,889] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:22:35,889] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:22:35,892] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:22:45,893] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:22:45,894] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:22:45,897] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:22:55,899] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:22:55,899] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:22:55,903] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:23:05,904] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:23:05,904] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:23:05,907] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:23:15,909] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:23:15,909] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:23:15,913] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:23:25,915] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:23:25,916] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:23:25,919] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:23:35,920] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:23:35,920] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:23:35,923] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:23:45,924] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:23:45,924] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:23:45,928] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:23:55,929] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:23:55,929] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:23:55,932] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:24:05,933] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:24:05,933] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:24:05,936] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:24:15,039] INFO [Worker clientId=connect-1, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1570)
[2020-12-10 11:24:15,939] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:24:15,939] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:24:15,942] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:24:25,943] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:24:25,943] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:24:25,947] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:24:35,948] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:24:35,948] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:24:35,951] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:24:45,952] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:24:45,952] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:24:45,955] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:24:55,956] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:24:55,957] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:24:55,961] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:25:05,961] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:25:05,962] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:25:05,966] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:25:15,967] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:25:15,968] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:25:15,971] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:25:25,972] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:25:25,972] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:25:25,975] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:25:35,976] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:25:35,977] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:25:35,979] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:25:45,981] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:25:45,981] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:25:45,984] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:25:55,986] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:25:55,986] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:25:55,989] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:26:05,990] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:26:05,990] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:26:05,994] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:26:15,996] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:26:15,997] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:26:16,000] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 2 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:26:26,000] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:26:26,000] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:26:26,004] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:26:36,005] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:26:36,005] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:26:36,008] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:26:46,009] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:26:46,010] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:26:46,014] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:26:56,015] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:26:56,015] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:26:56,018] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:27:06,020] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:27:06,020] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:27:06,028] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:27:16,029] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:27:16,029] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:27:16,033] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:27:26,035] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:27:26,036] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:27:26,043] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:27:36,044] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:27:36,044] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:27:36,047] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:27:46,048] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:27:46,048] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:27:46,051] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:27:56,053] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:27:56,053] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:27:56,056] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:28:06,057] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:28:06,057] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:28:06,061] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:28:16,063] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:28:16,064] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:28:16,067] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:28:26,069] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:28:26,069] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:28:26,072] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:28:36,073] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:28:36,074] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:28:36,100] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 26 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:28:46,101] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:28:46,102] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:28:46,105] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:28:56,106] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:28:56,106] INFO WorkerSourceTask{id=mqtt-source-0} flushing 1 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:28:56,113] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:29:06,114] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:29:06,114] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:29:06,120] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:29:13,345] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
org.apache.kafka.connect.errors.ConnectException: Failed to find any class that implements Connector and which name matches com.mongodb.kafka.connect.MongoSinkConnector, available connectors are: PluginDesc{klass=class io.confluent.connect.mqtt.MqttSinkConnector, name='io.confluent.connect.mqtt.MqttSinkConnector', version='0.0.0.0', encodedVersion=0.0.0.0, type=sink, typeName='sink', location='file:/usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0/'}, PluginDesc{klass=class io.confluent.connect.mqtt.MqttSourceConnector, name='io.confluent.connect.mqtt.MqttSourceConnector', version='0.0.0.0', encodedVersion=0.0.0.0, type=source, typeName='source', location='file:/usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0/'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSinkConnector, name='org.apache.kafka.connect.file.FileStreamSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSourceConnector, name='org.apache.kafka.connect.file.FileStreamSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockConnector, name='org.apache.kafka.connect.tools.MockConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=connector, typeName='connector', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSinkConnector, name='org.apache.kafka.connect.tools.MockSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSourceConnector, name='org.apache.kafka.connect.tools.MockSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.SchemaSourceConnector, name='org.apache.kafka.connect.tools.SchemaSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSinkConnector, name='org.apache.kafka.connect.tools.VerifiableSinkConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSourceConnector, name='org.apache.kafka.connect.tools.VerifiableSourceConnector', version='6.0.0-ccs', encodedVersion=6.0.0-ccs, type=source, typeName='source', location='classpath'}
	at org.apache.kafka.connect.runtime.isolation.Plugins.connectorClass(Plugins.java:208)
	at org.apache.kafka.connect.runtime.isolation.Plugins.newConnector(Plugins.java:180)
	at org.apache.kafka.connect.runtime.AbstractHerder.getConnector(AbstractHerder.java:572)
	at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:342)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$1(AbstractHerder.java:326)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2020-12-10 11:29:16,121] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:29:16,121] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:29:16,125] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:29:26,130] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:29:26,130] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:29:26,135] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:29:36,136] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:29:36,136] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:29:36,140] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:29:45,463] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:67)
[2020-12-10 11:29:45,463] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:327)
[2020-12-10 11:29:45,507] INFO Stopped http_8083@2bf94401{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:380)
[2020-12-10 11:29:45,508] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:158)
[2020-12-10 11:29:45,528] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:344)
[2020-12-10 11:29:45,529] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:676)
[2020-12-10 11:29:45,530] INFO [Worker clientId=connect-1, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:650)
[2020-12-10 11:29:45,534] INFO Stopping connector mqtt-source (org.apache.kafka.connect.runtime.Worker:387)
[2020-12-10 11:29:45,535] INFO Scheduled shutdown for WorkerConnector{id=mqtt-source} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2020-12-10 11:29:45,541] INFO Stopping task mqtt-source-0 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:29:45,548] INFO Completed shutdown for WorkerConnector{id=mqtt-source} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2020-12-10 11:29:45,624] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:29:45,625] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:29:45,630] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:29:45,630] INFO [Producer clientId=connector-producer-mqtt-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1189)
[2020-12-10 11:29:45,641] INFO [Worker clientId=connect-1, groupId=connect-cluster] Member connect-1-18f0e6f9-2fa0-4853-a640-e763fa08a3f7 sending LeaveGroup request to coordinator kali:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:29:45,656] WARN [Worker clientId=connect-1, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:986)
[2020-12-10 11:29:45,666] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:167)
[2020-12-10 11:29:45,675] INFO [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1189)
[2020-12-10 11:29:45,690] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:193)
[2020-12-10 11:29:45,690] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:285)
[2020-12-10 11:29:45,690] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:167)
[2020-12-10 11:29:45,694] INFO [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1189)
[2020-12-10 11:29:45,710] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:193)
[2020-12-10 11:29:45,711] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:287)
[2020-12-10 11:29:45,711] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:209)
[2020-12-10 11:29:45,712] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:134)
[2020-12-10 11:29:45,716] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:167)
[2020-12-10 11:29:45,717] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1189)
[2020-12-10 11:29:45,730] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:193)
[2020-12-10 11:29:45,730] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:136)
[2020-12-10 11:29:45,731] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:230)
[2020-12-10 11:29:45,735] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:299)
[2020-12-10 11:29:45,743] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:696)
[2020-12-10 11:29:45,743] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:72)
[2020-12-10 11:29:50,235] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/home/sudhir/confluent-6.0.0/bin/../logs, -Dlog4j.configuration=file:./bin/../etc/kafka/connect-log4j.properties
	jvm.spec = Debian, OpenJDK 64-Bit Server VM, 11.0.8, 11.0.8+10-post-Debian-1
	jvm.classpath = /home/sudhir/confluent-6.0.0/share/java/kafka/commons-cli-1.4.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/zookeeper-jute-3.5.8.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/scala-reflect-2.13.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-resolver-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jakarta.validation-api-2.0.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/hk2-utils-2.6.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/javassist-3.26.0-GA.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jakarta.activation-api-1.2.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/metrics-core-2.2.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/paranamer-2.8.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/reflections-0.9.12.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka-streams-scala_2.13-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/zstd-jni-1.4.4-7.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-util-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-transforms-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/lz4-java-1.7.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka-streams-test-utils-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka-clients-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/snappy-java-1.1.7.3.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/activation-1.1.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/audience-annotations-0.5.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/maven-artifact-3.6.3.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jersey-media-jaxb-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-buffer-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka_2.13-6.0.0-ccs-test.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-json-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-client-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-transport-native-unix-common-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/slf4j-log4j12-1.7.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka_2.13-6.0.0-ccs-test-sources.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-basic-auth-extension-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-module-paranamer-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka_2.13-6.0.0-ccs-scaladoc.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/commons-lang3-3.8.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/argparse4j-0.7.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jersey-client-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jersey-container-servlet-core-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka_2.13-6.0.0-ccs-sources.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/slf4j-api-1.7.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/javax.servlet-api-3.1.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-annotations-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-transport-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/hk2-api-2.6.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-transport-native-epoll-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka-streams-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka_2.13-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/plexus-utils-3.2.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-codec-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/hk2-locator-2.6.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-http-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka-log4j-appender-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-core-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-mirror-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jaxb-api-2.3.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-io-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-server-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/javassist-3.25.0-GA.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/javax.ws.rs-api-2.1.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/scala-library-2.13.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jersey-common-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/rocksdbjni-5.18.4.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/zookeeper-3.5.8.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jakarta.inject-2.6.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jersey-server-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-handler-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka_2.13-6.0.0-ccs-javadoc.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-api-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/scala-logging_2.13-3.9.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/netty-common-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/osgi-resource-locator-1.0.3.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jopt-simple-5.0.4.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/aopalliance-repackaged-2.6.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka-tools-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-mirror-client-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/kafka-streams-examples-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-file-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-security-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jersey-container-servlet-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jakarta.annotation-api-1.3.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/scala-collection-compat_2.13-2.1.6.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jersey-hk2-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-databind-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/connect-runtime-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/confluent-common/common-metrics-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/confluent-common/build-tools-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/confluent-common/slf4j-api-1.7.30.jar:/home/sudhir/confluent-6.0.0/share/java/confluent-common/common-utils-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/confluent-common/common-config-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-connect-avro-data-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/okio-2.5.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-protobuf-provider-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/handy-uri-templates-2.1.8.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/org.everit.json.schema-1.12.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-script-runtime-1.3.50.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/swagger-annotations-1.6.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/animal-sniffer-annotations-1.18.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-scripting-common-1.3.50.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-stdlib-jdk8-1.3.71.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/failureaccess-1.0.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/json-20190722.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-protobuf-serializer-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/commons-digester-1.8.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-scripting-compiler-embeddable-1.3.50.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-streams-protobuf-serde-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/avro-1.9.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/protobuf-java-3.11.4.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-json-schema-provider-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/protobuf-java-util-3.11.4.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-scripting-jvm-1.3.50.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlinx-coroutines-core-common-1.1.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jakarta.ws.rs-api-2.1.6.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-datatype-jdk8-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/commons-logging-1.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/validation-api-2.0.1.Final.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/classgraph-4.8.21.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-connect-json-schema-converter-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-datatype-jsr310-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-streams-json-schema-serde-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jsr305-3.0.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-annotations-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-datatype-guava-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/commons-compress-1.19.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-reflect-1.3.50.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-streams-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/wire-schema-3.2.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-json-schema-serializer-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-stdlib-jdk7-1.3.71.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/commons-collections-3.2.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-stdlib-1.4.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-core-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-connect-avro-converter-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlinx-coroutines-core-1.1.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/scala-library-2.13.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jersey-common-2.30.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/error_prone_annotations-2.3.4.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/rocksdbjni-5.18.4.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/guava-28.1-jre.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/re2j-1.3.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jakarta.inject-2.6.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-json-serializer-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/j2objc-annotations-1.3.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/commons-validator-1.6.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-schema-registry-client-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-module-parameter-names-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/osgi-resource-locator-1.0.3.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-datatype-joda-2.10.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/joda-time-2.9.9.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/wire-runtime-3.2.2.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-schema-serializer-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kotlin-stdlib-common-1.3.71.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/gson-2.8.6.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-avro-serializer-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-streams-avro-serde-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/checker-qual-2.8.1.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jakarta.annotation-api-1.3.5.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/kafka-connect-protobuf-converter-6.0.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/annotations-13.0.jar:/home/sudhir/confluent-6.0.0/share/java/kafka-serde-tools/jackson-databind-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/commons-cli-1.4.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/scala-reflect-2.13.2.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-resolver-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/paranamer-2.8.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/reflections-0.9.12.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka-streams-scala_2.13-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/zstd-jni-1.4.4-7.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-util-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-transforms-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/lz4-java-1.7.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka-streams-test-utils-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka-clients-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/activation-1.1.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jersey-media-jaxb-2.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-buffer-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka_2.13-6.0.0-ccs-test.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-json-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-client-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka_2.13-6.0.0-ccs-test-sources.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-basic-auth-extension-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-module-paranamer-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka_2.13-6.0.0-ccs-scaladoc.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jersey-client-2.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jersey-container-servlet-core-2.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka_2.13-6.0.0-ccs-sources.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-annotations-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-transport-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/hk2-api-2.6.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-transport-native-epoll-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka-streams-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka_2.13-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-codec-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-http-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka-log4j-appender-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-core-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-mirror-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-io-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-server-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/javassist-3.25.0-GA.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/scala-library-2.13.2.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jersey-common-2.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/rocksdbjni-5.18.4.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/zookeeper-3.5.8.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jersey-server-2.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-handler-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka_2.13-6.0.0-ccs-javadoc.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-api-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/scala-logging_2.13-3.9.2.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/netty-common-4.1.50.Final.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka-tools-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-mirror-client-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/kafka-streams-examples-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-file-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-security-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jersey-container-servlet-2.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/scala-collection-compat_2.13-2.1.6.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jersey-hk2-2.30.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-databind-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/connect-runtime-6.0.0-ccs.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/home/sudhir/confluent-6.0.0/bin/../share/java/confluent-telemetry/*
	os.spec = Linux, amd64, 5.7.0-kali1-amd64
	os.vcpus = 2
 (org.apache.kafka.connect.runtime.WorkerInfo:71)
[2020-12-10 11:29:50,279] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed:90)
[2020-12-10 11:29:50,359] INFO Loading plugin from: /usr/share/java/javatools.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2020-12-10 11:29:50,567] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/javatools.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2020-12-10 11:29:50,572] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:50,572] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:50,573] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:50,581] INFO Loading plugin from: /usr/share/java/mongodb-kafka-connect-mongodb-1.3.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2020-12-10 11:29:51,150] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/mongodb-kafka-connect-mongodb-1.3.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2020-12-10 11:29:51,150] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:51,151] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:51,151] INFO Loading plugin from: /usr/share/java/libintl.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2020-12-10 11:29:51,162] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/libintl.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2020-12-10 11:29:51,163] INFO Loading plugin from: /usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2020-12-10 11:29:52,262] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/confluentinc-kafka-connect-mqtt-1.4.0/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2020-12-10 11:29:52,263] INFO Added plugin 'io.confluent.connect.mqtt.MqttSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:52,266] INFO Added plugin 'io.confluent.connect.mqtt.MqttSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:52,268] INFO Loading plugin from: /usr/share/java/java-atk-wrapper.jar (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:246)
[2020-12-10 11:29:52,291] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/java/java-atk-wrapper.jar} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2020-12-10 11:29:58,595] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@75b84c92 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:269)
[2020-12-10 11:29:58,596] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,596] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,599] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,599] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,600] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,601] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,601] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,602] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,602] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,603] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,604] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,604] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,605] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,606] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,606] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,609] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,610] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,611] INFO Added plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,611] INFO Added plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,612] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,612] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,613] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,614] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,615] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,615] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,616] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,616] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,617] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,617] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,617] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,618] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,619] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,619] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,620] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,620] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,621] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,621] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,621] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,621] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,622] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,622] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,622] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,623] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,623] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,623] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,623] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,623] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,624] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,624] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,625] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,626] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:198)
[2020-12-10 11:29:58,627] INFO Added aliases 'MongoSinkConnector' and 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,628] INFO Added aliases 'MongoSourceConnector' and 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,628] INFO Added aliases 'MqttSinkConnector' and 'MqttSink' to plugin 'io.confluent.connect.mqtt.MqttSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,628] INFO Added aliases 'MqttSourceConnector' and 'MqttSource' to plugin 'io.confluent.connect.mqtt.MqttSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,629] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,629] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,629] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,629] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,630] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,630] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,630] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,631] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,631] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,631] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,631] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,633] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,633] INFO Added aliases 'JsonSchemaConverter' and 'JsonSchema' to plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,634] INFO Added aliases 'ProtobufConverter' and 'Protobuf' to plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,634] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,651] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,651] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,651] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,651] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,652] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,652] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,652] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,652] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,652] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,653] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,653] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,653] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,653] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,653] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,653] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,654] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,654] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,654] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,655] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,655] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,655] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,656] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,656] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,656] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,656] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:427)
[2020-12-10 11:29:58,656] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,657] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,657] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:430)
[2020-12-10 11:29:58,860] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 3
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = None
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	listeners = null
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 3
	offset.storage.topic = connect-offsets
	plugin.path = [/usr/share/java]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	rest.host.name = null
	rest.port = 8083
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 3
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:354)
[2020-12-10 11:29:58,870] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2020-12-10 11:29:58,878] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:29:59,013] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,015] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,016] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,016] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,017] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,017] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,017] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,017] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,018] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,018] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,018] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,018] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,018] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,020] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:29:59,021] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:29:59,021] INFO Kafka startTimeMs: 1607617799019 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:29:59,572] INFO Kafka cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.connect.util.ConnectUtils:65)
[2020-12-10 11:29:59,606] INFO Logging initialized @10690ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:169)
[2020-12-10 11:29:59,690] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:132)
[2020-12-10 11:29:59,691] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:204)
[2020-12-10 11:29:59,702] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 11.0.8+10-post-Debian-1 (org.eclipse.jetty.server.Server:359)
[2020-12-10 11:29:59,757] INFO Started http_8083@748e9b20{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:330)
[2020-12-10 11:29:59,758] INFO Started @10842ms (org.eclipse.jetty.server.Server:399)
[2020-12-10 11:29:59,820] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2020-12-10 11:29:59,820] INFO REST server listening at http://127.0.1.1:8083/, advertising URL http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2020-12-10 11:29:59,821] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2020-12-10 11:29:59,821] INFO REST admin endpoints at http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:220)
[2020-12-10 11:29:59,822] INFO Advertised URI: http://127.0.1.1:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:371)
[2020-12-10 11:29:59,826] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2020-12-10 11:29:59,827] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:29:59,854] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,855] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,855] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,855] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,856] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,856] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,856] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,856] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,856] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,856] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,856] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,857] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,857] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:29:59,857] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:29:59,858] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:29:59,858] INFO Kafka startTimeMs: 1607617799857 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:29:59,872] WARN [AdminClient clientId=adminclient-2] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:29:59,955] INFO Kafka cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.connect.util.ConnectUtils:65)
[2020-12-10 11:29:59,973] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy:45)
[2020-12-10 11:30:00,005] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2020-12-10 11:30:00,008] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:00,021] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,025] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,025] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,026] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,026] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,026] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,026] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,026] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,026] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,029] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,030] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,038] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,038] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,038] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,048] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,052] INFO Kafka startTimeMs: 1607617800038 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,121] INFO Kafka cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.connect.util.ConnectUtils:65)
[2020-12-10 11:30:00,144] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,145] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,145] INFO Kafka startTimeMs: 1607617800144 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,371] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:00,373] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:00,373] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2020-12-10 11:30:00,374] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:00,378] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,378] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,378] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,379] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,379] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,379] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,380] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,380] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,380] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,380] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,380] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,381] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,381] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,381] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,381] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,381] INFO Kafka startTimeMs: 1607617800381 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,434] INFO Kafka cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.connect.util.ConnectUtils:65)
[2020-12-10 11:30:00,450] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2020-12-10 11:30:00,451] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:00,454] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,455] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,458] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,458] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,458] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,458] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,458] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,459] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,459] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,459] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,459] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,459] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,460] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,460] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,460] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,460] INFO Kafka startTimeMs: 1607617800460 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,483] INFO Kafka cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.connect.util.ConnectUtils:65)
[2020-12-10 11:30:00,499] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2020-12-10 11:30:00,501] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:00,506] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,506] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,506] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,506] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,507] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,507] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,507] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,507] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,507] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,514] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,515] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,515] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,515] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,517] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,518] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,518] INFO Kafka startTimeMs: 1607617800517 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,539] INFO Kafka cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.connect.util.ConnectUtils:65)
[2020-12-10 11:30:00,564] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:49)
[2020-12-10 11:30:00,565] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:00,570] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,570] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,571] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,571] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,571] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,571] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,572] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,576] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,576] INFO Kafka startTimeMs: 1607617800572 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,581] WARN [AdminClient clientId=adminclient-7] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:00,604] INFO Kafka cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.connect.util.ConnectUtils:65)
[2020-12-10 11:30:00,655] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,655] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,655] INFO Kafka startTimeMs: 1607617800655 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,663] INFO Kafka Connect distributed worker initialization took 10384ms (org.apache.kafka.connect.cli.ConnectDistributed:128)
[2020-12-10 11:30:00,663] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:51)
[2020-12-10 11:30:00,666] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:224)
[2020-12-10 11:30:00,666] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:286)
[2020-12-10 11:30:00,666] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:195)
[2020-12-10 11:30:00,667] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:127)
[2020-12-10 11:30:00,667] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:127)
[2020-12-10 11:30:00,668] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:00,700] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,701] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,703] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,704] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,704] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,705] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,705] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,705] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,705] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,706] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,706] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,706] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,706] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,710] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,711] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:00,713] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:00,720] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:00,720] INFO Kafka startTimeMs: 1607617800711 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:00,736] WARN [AdminClient clientId=adminclient-8] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:00,867] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:241)
[2020-12-10 11:30:00,936] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:354)
[2020-12-10 11:30:01,046] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,047] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,048] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,048] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,048] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,048] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,048] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,048] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:01,048] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:01,048] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:01,049] INFO Kafka startTimeMs: 1607617801048 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:01,086] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:333)
[2020-12-10 11:30:01,097] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:338)
[2020-12-10 11:30:01,101] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:140)
[2020-12-10 11:30:01,105] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:01,126] INFO [Producer clientId=producer-1] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:01,246] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,250] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,250] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,250] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,250] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,251] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,251] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,251] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,252] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,252] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,252] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,252] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,252] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,252] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:01,252] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:01,255] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:01,255] INFO Kafka startTimeMs: 1607617801252 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:01,368] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:01,651] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Subscribed to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2020-12-10 11:30:01,656] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,657] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,658] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,661] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,663] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,663] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,664] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,664] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,664] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,664] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,694] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,694] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,694] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,694] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,694] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,694] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,695] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:01,820] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,828] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,835] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,835] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,835] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,836] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,836] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,836] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,836] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,837] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,837] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,837] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,844] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,845] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,850] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,851] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,855] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,859] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,862] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,863] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,863] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,864] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,866] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,868] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:01,869] INFO [Consumer clientId=consumer-connect-cluster-1, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:02,110] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:161)
[2020-12-10 11:30:02,111] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:163)
[2020-12-10 11:30:02,111] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:129)
[2020-12-10 11:30:02,130] INFO Worker started (org.apache.kafka.connect.runtime.Worker:202)
[2020-12-10 11:30:02,130] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:127)
[2020-12-10 11:30:02,131] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:02,133] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,137] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,138] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,138] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,141] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,145] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,146] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:02,146] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:02,146] INFO Kafka startTimeMs: 1607617802146 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:02,194] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:354)
[2020-12-10 11:30:02,202] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,203] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,204] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:02,204] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:02,204] INFO Kafka startTimeMs: 1607617802203 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:02,214] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:02,234] WARN [Producer clientId=producer-2] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:02,245] WARN [Producer clientId=producer-2] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:30:02,250] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,253] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,254] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,254] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,254] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,254] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,254] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,254] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,255] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,255] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,255] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,255] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,255] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,255] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,255] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:02,256] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:02,256] INFO Kafka startTimeMs: 1607617802255 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:02,285] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:02,318] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Subscribed to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2020-12-10 11:30:02,319] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:02,319] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:02,319] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:02,319] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:02,319] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:02,337] INFO [Producer clientId=producer-2] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:02,398] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:02,399] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:02,399] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:02,399] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:02,399] INFO [Consumer clientId=consumer-connect-cluster-2, groupId=connect-cluster] Resetting offset for partition connect-status-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:02,538] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:161)
[2020-12-10 11:30:02,539] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:163)
[2020-12-10 11:30:02,540] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:265)
[2020-12-10 11:30:02,540] INFO Starting KafkaBasedLog with topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:127)
[2020-12-10 11:30:02,541] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:02,548] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,548] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,550] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,551] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,560] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,562] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,565] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,565] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,567] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,567] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,570] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,570] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,570] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,570] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,571] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:02,571] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:02,571] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:02,573] INFO Kafka startTimeMs: 1607617802571 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:02,645] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-3
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:354)
[2020-12-10 11:30:02,650] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,650] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,650] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:02,651] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:02,651] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:02,651] INFO Kafka startTimeMs: 1607617802651 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:02,653] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-connect-cluster-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:02,675] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,676] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'offset.flush.interval.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,677] WARN The configuration 'value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,682] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,687] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,687] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,687] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:02,689] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:02,689] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:02,690] INFO Kafka startTimeMs: 1607617802689 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:02,697] INFO [Producer clientId=producer-3] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:02,717] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:02,747] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Subscribed to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2020-12-10 11:30:02,748] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Seeking to EARLIEST offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:02,773] INFO [Consumer clientId=consumer-connect-cluster-3, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:02,784] INFO Successfully processed removal of connector 'mqtt-source' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2020-12-10 11:30:02,806] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:161)
[2020-12-10 11:30:02,806] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:163)
[2020-12-10 11:30:02,810] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:280)
[2020-12-10 11:30:02,810] INFO [Worker clientId=connect-1, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:290)
[2020-12-10 11:30:02,828] WARN [Worker clientId=connect-1, groupId=connect-cluster] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:02,828] WARN [Worker clientId=connect-1, groupId=connect-cluster] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:30:02,949] INFO [Worker clientId=connect-1, groupId=connect-cluster] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:02,975] INFO [Worker clientId=connect-1, groupId=connect-cluster] Discovered group coordinator kali:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:03,005] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:30:03,006] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:03,063] INFO [Worker clientId=connect-1, groupId=connect-cluster] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:03,063] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:03,193] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:03,206] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=12, connectorIds=[mqtt-source], taskIds=[mqtt-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:30:03,212] WARN [Worker clientId=connect-1, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1089)
[2020-12-10 11:30:03,212] INFO [Worker clientId=connect-1, groupId=connect-cluster] Current config state offset -1 is behind group assignment 12, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1150)
[2020-12-10 11:30:03,289] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1154)
[2020-12-10 11:30:03,290] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:30:03,297] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector mqtt-source (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1291)
[2020-12-10 11:30:03,322] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mqtt-source-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:03,338] INFO Creating task mqtt-source-0 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:03,338] INFO Creating connector mqtt-source of type io.confluent.connect.mqtt.MqttSourceConnector (org.apache.kafka.connect.runtime.Worker:274)
[2020-12-10 11:30:03,347] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:354)
[2020-12-10 11:30:03,355] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:03,371] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:03,371] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:03,402] INFO TaskConfig values: 
	task.class = class io.confluent.connect.mqtt.MqttSourceTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:03,404] INFO Instantiated task mqtt-source-0 with version 0.0.0.0 of type io.confluent.connect.mqtt.MqttSourceTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:03,406] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:03,413] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mqtt-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:537)
[2020-12-10 11:30:03,407] INFO Instantiated connector mqtt-source with version 0.0.0.0 of type class io.confluent.connect.mqtt.MqttSourceConnector (org.apache.kafka.connect.runtime.Worker:284)
[2020-12-10 11:30:03,413] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:03,421] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mqtt-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:543)
[2020-12-10 11:30:03,421] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mqtt-source-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:03,415] INFO Finished creating connector mqtt-source (org.apache.kafka.connect.runtime.Worker:310)
[2020-12-10 11:30:03,455] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:354)
[2020-12-10 11:30:03,468] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:03,482] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:606)
[2020-12-10 11:30:03,491] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mqtt-source-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 2147483647
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:354)
[2020-12-10 11:30:03,531] INFO MqttSourceConnectorConfig values: 
	confluent.license = 
	confluent.topic = _confluent-command
	confluent.topic.bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	confluent.topic.replication.factor = 1
	kafka.topic = test6
	max.retry.time.ms = 30000
	mqtt.clean.session.enabled = true
	mqtt.connect.timeout.seconds = 30
	mqtt.keepalive.interval.seconds = 60
	mqtt.password = [hidden]
	mqtt.qos = 0
	mqtt.server.uri = [tcp://broker.alphatelecare.com:1884]
	mqtt.ssl.key.password = [hidden]
	mqtt.ssl.key.store.password = [hidden]
	mqtt.ssl.key.store.path = 
	mqtt.ssl.trust.store.password = [hidden]
	mqtt.ssl.trust.store.path = 
	mqtt.topics = [ION/+/CURRENTALARM]
	mqtt.username = admin
 (io.confluent.connect.mqtt.MqttSourceConnectorConfig:354)
[2020-12-10 11:30:03,538] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:03,589] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig:362)
[2020-12-10 11:30:03,589] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:03,590] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:03,590] INFO Kafka startTimeMs: 1607617803589 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:03,645] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:30:03,698] INFO [Producer clientId=connector-producer-mqtt-source-0] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:03,734] INFO Starting License Store (io.confluent.license.LicenseStore:244)
[2020-12-10 11:30:03,736] INFO Starting KafkaBasedLog with topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:127)
[2020-12-10 11:30:03,749] INFO AdminClientConfig values: 
	bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	client.dns.lookup = use_all_dns_ips
	client.id = mqtt-source-license-manager
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:354)
[2020-12-10 11:30:03,784] WARN The configuration 'replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig:362)
[2020-12-10 11:30:03,785] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:03,786] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:03,790] INFO Kafka startTimeMs: 1607617803785 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:03,816] INFO MqttSourceConnectorConfig values: 
	confluent.license = 
	confluent.topic = _confluent-command
	confluent.topic.bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	confluent.topic.replication.factor = 1
	kafka.topic = test6
	max.retry.time.ms = 30000
	mqtt.clean.session.enabled = true
	mqtt.connect.timeout.seconds = 30
	mqtt.keepalive.interval.seconds = 60
	mqtt.password = [hidden]
	mqtt.qos = 0
	mqtt.server.uri = [tcp://broker.alphatelecare.com:1884]
	mqtt.ssl.key.password = [hidden]
	mqtt.ssl.key.store.password = [hidden]
	mqtt.ssl.key.store.path = 
	mqtt.ssl.trust.store.password = [hidden]
	mqtt.ssl.trust.store.path = 
	mqtt.topics = [ION/+/CURRENTALARM]
	mqtt.username = admin
 (io.confluent.connect.mqtt.MqttSourceConnectorConfig:354)
[2020-12-10 11:30:03,883] INFO [Worker clientId=connect-1, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1570)
[2020-12-10 11:30:03,938] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = mqtt-source-license-manager
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.producer.ProducerConfig:354)
[2020-12-10 11:30:03,959] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:03,959] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:03,959] INFO Kafka startTimeMs: 1607617803959 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:03,960] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = mqtt-source-license-manager
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:03,993] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:03,994] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:03,994] INFO Kafka startTimeMs: 1607617803968 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:04,001] INFO [Consumer clientId=mqtt-source-license-manager, groupId=null] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:04,021] INFO [Producer clientId=mqtt-source-license-manager] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:04,058] INFO [Consumer clientId=mqtt-source-license-manager, groupId=null] Subscribed to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.KafkaConsumer:1120)
[2020-12-10 11:30:04,061] INFO [Consumer clientId=mqtt-source-license-manager, groupId=null] Seeking to EARLIEST offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:614)
[2020-12-10 11:30:04,095] INFO [Consumer clientId=mqtt-source-license-manager, groupId=null] Resetting offset for partition _confluent-command-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:04,112] INFO Started o.e.j.s.ServletContextHandler@6403e24c{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:825)
[2020-12-10 11:30:04,112] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:319)
[2020-12-10 11:30:04,112] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:57)
[2020-12-10 11:30:04,145] INFO Connecting to MQTT server. (io.confluent.connect.mqtt.MqttSourceTask:90)
[2020-12-10 11:30:04,201] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:161)
[2020-12-10 11:30:04,201] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:163)
[2020-12-10 11:30:04,201] INFO Started License Store (io.confluent.license.LicenseStore:246)
[2020-12-10 11:30:04,201] INFO Validating Confluent License (io.confluent.connect.utils.licensing.ConnectLicenseManager:256)
[2020-12-10 11:30:04,895] INFO Trial license for Confluent Enterprise expires in 29 days on 2021-01-09. (io.confluent.license.LicenseManager:503)
[2020-12-10 11:30:04,897] INFO Closing License Store (io.confluent.license.LicenseStore:252)
[2020-12-10 11:30:04,897] INFO Stopping KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:167)
[2020-12-10 11:30:04,898] INFO [Producer clientId=mqtt-source-license-manager] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1189)
[2020-12-10 11:30:04,904] INFO Stopped KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog:193)
[2020-12-10 11:30:04,904] INFO Closed License Store (io.confluent.license.LicenseStore:254)
[2020-12-10 11:30:04,915] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:354)
[2020-12-10 11:30:04,916] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.confluent.connect.mqtt.MqttSourceConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mqtt-source
	predicates = []
	tasks.max = 1
	topic.creation.groups = []
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:04,978] INFO Subscribing to ION/+/CURRENTALARM with QOS of 0 (io.confluent.connect.mqtt.MqttSourceTask:103)
[2020-12-10 11:30:05,084] INFO WorkerSourceTask{id=mqtt-source-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:233)
[2020-12-10 11:30:13,678] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:30:13,679] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:30:13,703] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 24 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:30:17,175] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:17,189] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:17,243] INFO Cluster created with settings {hosts=[18.140.231.254:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500} (org.mongodb.driver.cluster:71)
[2020-12-10 11:30:17,693] INFO Opened connection [connectionId{localValue:1, serverValue:184983}] to 18.140.231.254:27017 (org.mongodb.driver.connection:71)
[2020-12-10 11:30:17,813] INFO Monitor thread successfully connected to server with description ServerDescription{address=18.140.231.254:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, version=ServerVersion{versionList=[4, 4, 1]}, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=102740801, setName='app1r1', canonicalAddress=ip-172-31-1-240:27017, hosts=[ip-172-31-1-240:27017], passives=[], arbiters=[], primary='ip-172-31-1-240:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, lastWriteDate=Thu Dec 10 11:30:20 EST 2020, lastUpdateTimeNanos=34128638491815} (org.mongodb.driver.cluster:71)
[2020-12-10 11:30:18,693] INFO Opened connection [connectionId{localValue:2, serverValue:184984}] to 18.140.231.254:27017 (org.mongodb.driver.connection:71)
[2020-12-10 11:30:18,965] INFO Closed connection [connectionId{localValue:2, serverValue:184984}] to 18.140.231.254:27017 because the pool has been closed. (org.mongodb.driver.connection:71)
[2020-12-10 11:30:18,970] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:354)
[2020-12-10 11:30:18,989] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector mongo-sink config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1526)
[2020-12-10 11:30:19,495] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:30:19,495] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:19,503] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:19,504] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 10 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=14, connectorIds=[mongo-sink, mqtt-source], taskIds=[mqtt-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:30:19,509] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 14 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:30:19,510] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector mongo-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1291)
[2020-12-10 11:30:19,511] INFO Creating connector mongo-sink of type com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2020-12-10 11:30:19,513] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:19,524] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:19,527] INFO Instantiated connector mongo-sink with version 1.3.0 of type class com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2020-12-10 11:30:19,537] INFO Finished creating connector mongo-sink (org.apache.kafka.connect.runtime.Worker:310)
[2020-12-10 11:30:19,537] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:30:19,547] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:19,556] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,055] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [mongo-sink-8, mongo-sink-9, mongo-sink-10, mongo-sink-11, mongo-sink-4, mongo-sink-5, mongo-sink-6, mongo-sink-7, mongo-sink-0, mongo-sink-1, mongo-sink-2, mongo-sink-3, mongo-sink-16, mongo-sink-17, mongo-sink-18, mongo-sink-19, mongo-sink-12, mongo-sink-13, mongo-sink-14, mongo-sink-15] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1541)
[2020-12-10 11:30:20,557] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:641)
[2020-12-10 11:30:20,558] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:30:20,558] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:20,566] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:20,567] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 11 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=35, connectorIds=[mongo-sink, mqtt-source], taskIds=[mongo-sink-0, mongo-sink-1, mongo-sink-2, mongo-sink-3, mongo-sink-4, mongo-sink-5, mongo-sink-6, mongo-sink-7, mongo-sink-8, mongo-sink-9, mongo-sink-10, mongo-sink-11, mongo-sink-12, mongo-sink-13, mongo-sink-14, mongo-sink-15, mongo-sink-16, mongo-sink-17, mongo-sink-18, mongo-sink-19, mqtt-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:30:20,568] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 35 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:30:20,569] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-8 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,569] INFO Creating task mongo-sink-8 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,574] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-9 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,575] INFO Creating task mongo-sink-9 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,576] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-10 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,576] INFO Creating task mongo-sink-10 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,582] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-11 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,583] INFO Creating task mongo-sink-11 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,582] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,586] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,587] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,583] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,592] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,589] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,589] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,595] INFO Creating task mongo-sink-7 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,596] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,597] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,598] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,589] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-5 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,599] INFO Creating task mongo-sink-5 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,589] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-6 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,600] INFO Creating task mongo-sink-6 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,595] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,593] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,592] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,591] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,601] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,601] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,609] INFO Creating task mongo-sink-4 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,610] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,611] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,611] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,611] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,612] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,612] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,612] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,612] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,613] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,613] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,614] INFO Instantiated task mongo-sink-5 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,614] INFO Instantiated task mongo-sink-9 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,614] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,615] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,615] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-5 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,615] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-5 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,615] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-5 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,617] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,626] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,627] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,617] INFO Instantiated task mongo-sink-6 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,628] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,628] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,629] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-6 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,629] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-6 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,629] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-6 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,630] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,631] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,625] INFO Instantiated task mongo-sink-4 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,625] INFO Instantiated task mongo-sink-8 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,632] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,633] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,634] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-8 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,634] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-8 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,634] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-8 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,635] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,636] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,636] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,637] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,625] INFO Instantiated task mongo-sink-11 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,638] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,638] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,638] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-11 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,638] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-11 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,638] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-11 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,639] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,639] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,640] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,640] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-11
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,622] INFO Instantiated task mongo-sink-7 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,622] INFO Instantiated task mongo-sink-10 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,614] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,642] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,643] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,643] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-9 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,643] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-9 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,643] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-9 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,637] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,645] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,645] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-4 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,645] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-4 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,646] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-4 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,632] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,647] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,645] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,643] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,650] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-10 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,650] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-10 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,650] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-10 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,648] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,652] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,652] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-7 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,652] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-7 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,653] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-7 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,653] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,654] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,654] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,654] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,654] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,655] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,671] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,671] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,674] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,674] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,675] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,676] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-10
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,677] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,677] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,678] INFO Kafka startTimeMs: 1607617820654 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,679] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,679] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,693] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,694] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,694] INFO Kafka startTimeMs: 1607617820679 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,694] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,694] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,694] INFO Kafka startTimeMs: 1607617820671 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,697] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,698] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,698] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,699] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,699] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,699] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,699] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,699] INFO Kafka startTimeMs: 1607617820699 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,699] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,703] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,703] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,704] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,704] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,704] INFO Kafka startTimeMs: 1607617820704 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,705] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,705] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,706] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,707] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,711] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,711] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,711] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,711] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,711] INFO Kafka startTimeMs: 1607617820711 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,713] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,713] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,713] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,714] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,714] INFO Kafka startTimeMs: 1607617820713 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,714] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,714] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,719] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,719] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,719] INFO Kafka startTimeMs: 1607617820715 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,720] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,720] INFO Creating task mongo-sink-0 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,720] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,724] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,724] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,724] INFO Instantiated task mongo-sink-0 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,724] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,725] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,725] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,725] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,726] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,727] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,727] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,728] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,729] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,730] INFO Creating task mongo-sink-1 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,730] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,730] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,731] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,732] INFO Creating task mongo-sink-2 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,732] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,733] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,734] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,731] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,734] INFO Instantiated task mongo-sink-1 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,734] INFO Instantiated task mongo-sink-2 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,734] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,734] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,734] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,735] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,735] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-2 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,735] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-2 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,734] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,735] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-1 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,735] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-2 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,737] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,736] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-1 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,735] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-3 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,738] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-1 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,738] INFO Creating task mongo-sink-3 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,738] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,739] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,739] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,739] INFO Instantiated task mongo-sink-3 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,739] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,740] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,740] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-3 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,739] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,740] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-3 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,740] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-3 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,740] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,741] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,742] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,742] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,743] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,747] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,743] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,748] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,749] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,750] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,760] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,761] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-16 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,774] INFO Creating task mongo-sink-16 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,775] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,776] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,803] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-18 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,830] INFO Creating task mongo-sink-18 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,831] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,832] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,832] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,832] INFO Instantiated task mongo-sink-18 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,833] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,802] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-17 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,834] INFO Creating task mongo-sink-17 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,835] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,835] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,836] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,836] INFO Instantiated task mongo-sink-17 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,836] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,794] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,848] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,782] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,849] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,849] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,848] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,856] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,857] INFO Kafka startTimeMs: 1607617820848 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,848] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,859] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-18 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,859] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-18 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,859] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-18 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,847] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,868] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,846] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,831] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-19 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,869] INFO Creating task mongo-sink-19 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,869] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,870] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,830] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,870] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,871] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,870] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,878] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,868] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-17 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,858] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,884] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,884] INFO Kafka startTimeMs: 1607617820848 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,884] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-17 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,871] INFO Instantiated task mongo-sink-16 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,894] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,893] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-17 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,912] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:20,907] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,901] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,913] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-16 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,914] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-16 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,902] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,915] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-16 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,913] INFO Creating task mongo-sink-12 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,915] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,916] INFO Kafka startTimeMs: 1607617820849 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,916] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:20,917] INFO Instantiated task mongo-sink-19 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,917] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:20,918] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,918] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,917] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,919] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,919] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,919] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-19 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,918] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:20,918] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:20,920] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-18
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,918] INFO Kafka startTimeMs: 1607617820868 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,921] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,919] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-19 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,922] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-19 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,924] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,924] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,919] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,926] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,926] INFO Instantiated task mongo-sink-12 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,926] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,927] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,928] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-16
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,921] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,939] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,939] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-17
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,937] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-14 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,935] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-13 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,941] INFO Creating task mongo-sink-14 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,929] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,929] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,942] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,942] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,927] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,943] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-12 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,943] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-12 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,943] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,944] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-19
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,942] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,942] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,941] INFO Creating task mongo-sink-13 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,947] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:20,947] INFO Kafka startTimeMs: 1607617820942 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,947] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,943] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-12 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,947] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,947] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,948] INFO Instantiated task mongo-sink-13 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,948] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,948] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,948] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-13 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,948] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-13 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,948] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,948] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-13 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,949] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,949] INFO Instantiated task mongo-sink-14 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,949] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,950] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,950] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-14 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,950] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-14 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,950] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-14 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,952] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,953] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,954] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,954] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-12
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,959] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:20,960] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:20,961] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,961] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-13
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,966] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-15 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:30:20,967] INFO Creating task mongo-sink-15 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:30:20,968] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:30:20,968] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:20,984] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:30:20,985] INFO Instantiated task mongo-sink-15 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:30:20,985] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:30:20,985] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:30:20,986] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-15 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:30:20,986] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-15 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:30:20,981] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,978] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:20,978] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:20,990] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:20,978] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,002] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,002] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,002] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:20,978] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:20,978] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,006] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:21,006] INFO Kafka startTimeMs: 1607617821002 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:20,978] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,009] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,010] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,010] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:21,010] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:21,003] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:21,014] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:21,015] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:21,015] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-14
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:21,002] INFO WorkerSinkTask{id=mongo-sink-6} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,000] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,018] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:20,998] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-15 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:30:20,988] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,011] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,019] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,021] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:30:21,021] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:30:21,029] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 20
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:30:21,030] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-15
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:30:20,978] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:20,978] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,024] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,023] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,047] INFO Kafka startTimeMs: 1607617821010 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:21,043] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,043] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,041] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,010] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,008] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,041] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,052] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,078] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,079] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,061] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,081] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,082] INFO WorkerSinkTask{id=mongo-sink-5} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,082] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:21,083] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:21,083] INFO Kafka startTimeMs: 1607617821079 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:21,079] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:30:21,084] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:21,085] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:21,085] INFO Kafka startTimeMs: 1607617821020 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:21,087] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:21,088] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:21,088] INFO Kafka startTimeMs: 1607617821019 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:21,090] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:21,090] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:21,090] INFO Kafka startTimeMs: 1607617821018 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:21,090] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,092] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:30:21,092] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:30:21,092] INFO Kafka startTimeMs: 1607617821084 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:30:21,093] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,095] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,096] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,096] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,054] INFO WorkerSinkTask{id=mongo-sink-7} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,055] INFO WorkerSinkTask{id=mongo-sink-9} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,060] INFO WorkerSinkTask{id=mongo-sink-4} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,060] INFO WorkerSinkTask{id=mongo-sink-11} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,061] INFO WorkerSinkTask{id=mongo-sink-10} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,061] INFO WorkerSinkTask{id=mongo-sink-8} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,100] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,124] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,124] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,143] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Finished assignment for group at generation 1: {connector-consumer-mongo-sink-6-5f39da8d-3fb5-45a2-a212-c9cf90eb807f=Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:21,155] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,156] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,156] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,156] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,156] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,180] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,201] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,201] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,202] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:30:21,211] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,211] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,211] INFO WorkerSinkTask{id=mongo-sink-18} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,211] INFO WorkerSinkTask{id=mongo-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,214] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,226] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,227] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,228] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,228] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,228] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,229] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,229] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,230] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,230] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,234] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,245] INFO WorkerSinkTask{id=mongo-sink-3} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,237] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,247] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,247] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,237] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,249] INFO WorkerSinkTask{id=mongo-sink-2} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,235] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,251] INFO WorkerSinkTask{id=mongo-sink-1} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,253] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,254] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:21,254] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:30:21,214] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,278] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,290] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,306] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,306] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,306] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,307] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,309] INFO WorkerSinkTask{id=mongo-sink-16} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,310] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,311] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,311] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,311] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,312] WARN [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:21,312] WARN [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:30:21,313] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:30:21,318] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,318] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,331] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,387] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,385] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,388] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,384] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,390] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,391] INFO WorkerSinkTask{id=mongo-sink-14} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,383] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,395] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,383] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,404] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,404] INFO WorkerSinkTask{id=mongo-sink-17} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,383] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,406] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,381] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,381] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,381] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,408] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,380] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,410] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,371] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,371] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:30:21,412] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,413] INFO WorkerSinkTask{id=mongo-sink-19} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,339] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,336] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,418] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,418] INFO WorkerSinkTask{id=mongo-sink-15} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,421] INFO WorkerSinkTask{id=mongo-sink-13} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,424] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,413] WARN [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:21,401] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,431] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,432] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,432] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,398] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,433] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,387] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,387] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,435] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,433] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,437] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,431] WARN [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:30:21,439] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:30:21,440] INFO WorkerSinkTask{id=mongo-sink-12} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:30:21,442] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,444] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,448] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,449] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,449] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,450] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,454] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,456] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,466] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,467] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,469] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,469] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,481] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,486] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,486] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,495] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,506] WARN [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:30:21,506] WARN [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:30:21,518] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,518] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,527] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,531] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,532] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,533] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,536] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,535] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,539] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,534] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,542] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,544] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,545] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,545] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,545] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,545] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,539] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,545] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,552] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,565] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,570] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,570] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,573] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,578] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,578] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,582] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,586] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,587] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,587] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,598] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,599] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,599] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,599] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,603] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,616] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:30:21,623] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:30:21,624] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,624] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,630] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,630] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,630] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,632] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,647] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,657] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,657] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,657] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:30:21,658] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:21,713] ERROR WorkerSinkTask{id=mongo-sink-6} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:21,715] ERROR WorkerSinkTask{id=mongo-sink-6} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:21,715] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:21,716] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:21,716] INFO [Consumer clientId=connector-consumer-mongo-sink-6, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-6-5f39da8d-3fb5-45a2-a212-c9cf90eb807f sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:21,725] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 2: {connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-12-1bd590f0-6684-43bd-80a7-03268b301d99=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-16-4f9a6c1b-8768-41ce-96f7-2d414b73d825=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-14-468dd70f-c98a-481e-add4-b933ded4f11c=Assignment(partitions=[]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-13-d767b639-c2c6-4dfd-b30d-262378e24665=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-15-891dc8c4-8998-4e86-92ea-910ee6fab532=Assignment(partitions=[]), connector-consumer-mongo-sink-11-d7dbac95-0998-478f-bc2e-fb2d7ae60747=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:21,775] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,775] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,775] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,776] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,777] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,777] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,777] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,776] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,776] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,779] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,776] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,779] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,779] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,780] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,778] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,782] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,782] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,783] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,783] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,784] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,784] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,778] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,777] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,786] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,777] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,786] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,782] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,780] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,788] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,788] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,790] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,790] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,792] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,792] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,792] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,792] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,793] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,793] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,794] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,794] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,794] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,795] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,795] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,796] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,799] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,799] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,801] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,801] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,807] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,807] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,807] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,808] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,809] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,809] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,811] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 2 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:21,812] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:21,812] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:21,813] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,813] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,815] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,815] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,814] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:21,826] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,827] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,831] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,833] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,839] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:21,854] ERROR WorkerSinkTask{id=mongo-sink-11} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:21,854] ERROR WorkerSinkTask{id=mongo-sink-11} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:21,855] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:21,855] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:21,856] INFO [Consumer clientId=connector-consumer-mongo-sink-11, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-11-d7dbac95-0998-478f-bc2e-fb2d7ae60747 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:23,704] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:30:23,705] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:30:23,711] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:30:24,809] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,810] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,813] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,818] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,818] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,819] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,820] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,820] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,821] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,821] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,821] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,822] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,822] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,821] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,823] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,823] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,823] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,824] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,824] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,822] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,826] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,827] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,827] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,828] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,830] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,830] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,830] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,830] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,831] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,831] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,833] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,833] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,833] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,834] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,834] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,834] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,834] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,835] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,835] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,839] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,842] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,842] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,841] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,841] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,843] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,844] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,841] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,839] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:24,847] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,847] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,846] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,847] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,850] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,850] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:24,867] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 3: {connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-12-1bd590f0-6684-43bd-80a7-03268b301d99=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-16-4f9a6c1b-8768-41ce-96f7-2d414b73d825=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-14-468dd70f-c98a-481e-add4-b933ded4f11c=Assignment(partitions=[]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-13-d767b639-c2c6-4dfd-b30d-262378e24665=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-15-891dc8c4-8998-4e86-92ea-910ee6fab532=Assignment(partitions=[]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:24,875] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,876] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,876] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,876] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,876] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,878] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,879] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,880] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,880] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,885] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,886] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,886] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,887] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,887] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,888] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,888] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,889] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,889] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,890] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,890] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,890] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,891] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,891] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,891] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,891] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,892] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,892] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,893] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,893] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,893] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,894] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,894] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,895] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,896] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,896] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,896] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:24,897] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:24,898] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,899] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,900] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,900] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,901] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,901] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:24,901] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,902] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,903] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,901] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:24,903] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Successfully joined group with generation 3 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:24,904] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,904] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,906] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,906] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,907] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,907] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,903] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,908] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,909] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:24,909] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:24,910] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:24,910] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:24,911] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:24,913] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:24,915] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:24,921] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:24,940] ERROR WorkerSinkTask{id=mongo-sink-12} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:24,946] ERROR WorkerSinkTask{id=mongo-sink-12} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:24,946] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:24,948] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:24,948] INFO [Consumer clientId=connector-consumer-mongo-sink-12, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-12-1bd590f0-6684-43bd-80a7-03268b301d99 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:27,878] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,879] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,879] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,879] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,879] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,880] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,882] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,882] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,882] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,898] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,899] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,899] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,902] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,904] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,904] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,906] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,907] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,907] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,909] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,909] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,909] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,910] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,911] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,912] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,913] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,914] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,914] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,914] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,915] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,915] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,915] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,916] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,916] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,917] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,917] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,917] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,920] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,920] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,918] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,921] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,922] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,922] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,922] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,923] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,922] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,927] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,930] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:27,930] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,930] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,930] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:27,932] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:27,943] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 4: {connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-16-4f9a6c1b-8768-41ce-96f7-2d414b73d825=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-14-468dd70f-c98a-481e-add4-b933ded4f11c=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-13-d767b639-c2c6-4dfd-b30d-262378e24665=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-15-891dc8c4-8998-4e86-92ea-910ee6fab532=Assignment(partitions=[]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:27,953] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,953] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,953] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,954] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,954] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,955] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,955] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,956] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,956] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,954] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,956] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,957] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,957] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,957] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,959] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,956] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,955] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,960] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,960] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,961] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,961] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,954] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,962] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,962] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,959] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:28,004] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:28,005] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:28,006] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:28,006] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,958] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:28,006] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:28,007] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,957] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:28,007] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:28,005] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,966] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:28,009] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,965] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,964] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,964] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:28,010] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:28,010] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:27,964] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:27,963] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 4 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:27,963] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:28,012] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:28,012] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:28,014] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:28,014] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:28,014] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:28,015] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:28,015] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:28,090] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:28,091] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:28,123] ERROR WorkerSinkTask{id=mongo-sink-13} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:28,124] ERROR WorkerSinkTask{id=mongo-sink-13} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:28,124] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:28,125] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:28,126] INFO [Consumer clientId=connector-consumer-mongo-sink-13, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-13-d767b639-c2c6-4dfd-b30d-262378e24665 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:30,965] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,966] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,966] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,972] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,972] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,972] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,973] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,975] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,976] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,975] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,976] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,975] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,977] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,977] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,975] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,974] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,974] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,978] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,979] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,979] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,980] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,978] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,981] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,982] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,975] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,985] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,985] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,975] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,986] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,986] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:30,975] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:30,987] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:30,987] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:31,019] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:31,019] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:31,019] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:31,019] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:31,019] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:31,020] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:31,021] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:31,022] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:31,023] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:31,024] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:31,025] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:31,025] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:31,025] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:31,026] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:31,026] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:31,038] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 5: {connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-16-4f9a6c1b-8768-41ce-96f7-2d414b73d825=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-14-468dd70f-c98a-481e-add4-b933ded4f11c=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-15-891dc8c4-8998-4e86-92ea-910ee6fab532=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:31,051] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,051] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,051] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,052] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,052] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,052] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,053] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,053] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,053] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,054] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,055] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,055] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,057] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,057] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,057] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,058] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,058] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,058] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,059] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,060] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,060] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,061] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,062] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,062] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,063] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,064] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,064] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,066] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,067] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,067] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,067] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,067] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,068] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,068] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,068] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,068] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,069] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,069] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,070] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,070] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,070] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,070] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,071] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,071] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,071] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,072] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Successfully joined group with generation 5 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:31,073] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:31,074] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:31,081] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:31,081] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:31,085] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:31,087] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:31,085] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:31,087] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:31,095] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:31,096] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:31,098] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:31,096] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:31,143] ERROR WorkerSinkTask{id=mongo-sink-14} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:31,145] ERROR WorkerSinkTask{id=mongo-sink-14} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:31,145] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:31,150] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:31,151] INFO [Consumer clientId=connector-consumer-mongo-sink-14, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-14-468dd70f-c98a-481e-add4-b933ded4f11c sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:33,713] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:30:33,714] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:30:33,719] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:30:34,069] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,070] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,079] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,080] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,080] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,081] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,081] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,083] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,084] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,084] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,084] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,084] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,084] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,084] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,084] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,086] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,087] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,088] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,088] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,088] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,088] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,088] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,091] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,091] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,091] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,091] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,091] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,091] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,092] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,092] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,092] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,092] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,092] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,093] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,092] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,095] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,096] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,097] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,097] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,097] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:34,098] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,098] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,099] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,092] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,100] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:34,110] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 6: {connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-16-4f9a6c1b-8768-41ce-96f7-2d414b73d825=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-15-891dc8c4-8998-4e86-92ea-910ee6fab532=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:34,116] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,117] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,117] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,117] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,117] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,117] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,119] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,119] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,117] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,120] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,120] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,120] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,120] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,119] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,121] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,121] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,119] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,122] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,122] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,119] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,122] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,120] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,123] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,123] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,124] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:34,128] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:34,129] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,130] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,130] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,130] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,131] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,132] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,133] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:34,134] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,135] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,135] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,136] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,136] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,140] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,140] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:34,141] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:34,143] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,144] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,144] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,144] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:34,148] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,149] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,149] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,149] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:34,152] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:34,153] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:34,154] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:34,155] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 6 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:34,155] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:34,156] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:34,172] ERROR WorkerSinkTask{id=mongo-sink-15} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:34,173] ERROR WorkerSinkTask{id=mongo-sink-15} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:34,173] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:34,179] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:34,180] INFO [Consumer clientId=connector-consumer-mongo-sink-15, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-15-891dc8c4-8998-4e86-92ea-910ee6fab532 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:37,118] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,118] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,119] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,123] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,125] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,124] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,125] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,126] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,124] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,127] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,127] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,125] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,128] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,128] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,128] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,129] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,130] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,130] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,131] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,131] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,135] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,135] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,135] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,139] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,139] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,140] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,144] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,144] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,144] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,150] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,151] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,151] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,130] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,155] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,155] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,155] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,160] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,160] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:37,161] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,161] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,165] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,165] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:37,168] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 7: {connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-16-4f9a6c1b-8768-41ce-96f7-2d414b73d825=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:37,173] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,173] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,173] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,174] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,174] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,174] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,174] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,174] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,174] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,175] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,173] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,177] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,177] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,177] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,177] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,177] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,178] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,176] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,179] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,179] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,176] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,179] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,180] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,177] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,180] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,180] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,177] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,181] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,181] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,181] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,182] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:37,182] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:37,183] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:37,184] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,184] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:37,186] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,186] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,186] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,187] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:37,188] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:37,190] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:37,185] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,191] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,190] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:37,197] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:37,199] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,199] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,199] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,200] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 7 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:37,207] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:37,207] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:37,208] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:37,218] ERROR WorkerSinkTask{id=mongo-sink-16} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:37,248] ERROR WorkerSinkTask{id=mongo-sink-16} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:37,248] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:37,249] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:37,249] INFO [Consumer clientId=connector-consumer-mongo-sink-16, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-16-4f9a6c1b-8768-41ce-96f7-2d414b73d825 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:40,188] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,192] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,192] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,192] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,193] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,193] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,193] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,191] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,194] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,194] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,192] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,195] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,196] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,196] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,196] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,196] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,196] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,196] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,197] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,197] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,197] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,197] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,198] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,198] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,196] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,201] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,198] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,202] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,197] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,202] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,202] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,202] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,206] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,207] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,207] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,202] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,202] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:40,208] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,208] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:40,211] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 8: {connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:40,217] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,217] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,218] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,217] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,218] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,219] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,219] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,221] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,218] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,217] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,221] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,221] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,221] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,221] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,222] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,221] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,220] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,220] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:40,220] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,223] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,223] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,220] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,224] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,224] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,220] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,220] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 8 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:40,220] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,225] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,219] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,227] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,225] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,225] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,227] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,223] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,227] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,221] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,227] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,228] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,231] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:40,231] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:40,231] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:40,231] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:40,232] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:40,234] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:40,236] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:40,238] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:40,243] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:40,245] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:40,251] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:40,262] ERROR WorkerSinkTask{id=mongo-sink-17} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:40,263] ERROR WorkerSinkTask{id=mongo-sink-17} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:40,263] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:40,264] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:40,264] INFO [Consumer clientId=connector-consumer-mongo-sink-17, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-17-e3884189-7407-4927-8c3b-fa5ed0a2fc7c sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:43,220] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,221] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,221] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,235] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,235] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,236] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,237] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,237] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,238] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,239] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,239] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,240] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,239] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,240] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,240] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,239] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,242] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,242] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,242] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,244] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,245] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,244] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,247] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,252] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,252] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,254] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,255] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,263] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,263] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,263] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,255] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,265] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,264] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:43,269] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,269] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,269] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:43,274] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Finished assignment for group at generation 9: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:43,284] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,285] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,285] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,286] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,286] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,285] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,289] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,290] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,290] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,285] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,290] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,291] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,291] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,291] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,290] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,291] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,290] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,292] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,290] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,293] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,293] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,295] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:43,296] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:43,297] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:43,299] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:43,300] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,302] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,302] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,300] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,303] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,304] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,301] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:43,306] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:43,306] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:43,308] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,308] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,309] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,308] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,310] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,312] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,308] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:43,311] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 9 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:43,313] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:43,314] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:43,315] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:43,318] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:43,333] ERROR WorkerSinkTask{id=mongo-sink-18} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:43,334] ERROR WorkerSinkTask{id=mongo-sink-18} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:43,334] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:43,338] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:43,338] INFO [Consumer clientId=connector-consumer-mongo-sink-18, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-18-fe803bb4-8f05-4cf2-853b-a702c20ed8c8 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:43,720] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:30:43,721] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:30:43,726] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:30:46,288] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,288] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,288] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,288] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,288] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,288] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,289] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,288] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,289] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,292] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,292] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,292] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,295] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,295] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,295] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,295] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,296] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,296] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,303] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,304] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,304] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,305] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,305] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,306] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,310] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,310] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,311] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,325] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,325] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,325] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,326] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:46,326] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,326] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:46,330] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Finished assignment for group at generation 10: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:46,334] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,335] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,335] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,335] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,335] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,336] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,336] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,336] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,336] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,336] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,336] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,336] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,337] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,338] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,338] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,338] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,338] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,338] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,339] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,338] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,341] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,341] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,341] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,341] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:46,340] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:46,343] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:46,341] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:46,344] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:46,344] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 10 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:46,348] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:46,344] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:46,349] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:46,351] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:46,351] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:46,351] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:46,352] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:46,368] ERROR WorkerSinkTask{id=mongo-sink-19} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:46,371] ERROR WorkerSinkTask{id=mongo-sink-19} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:46,371] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:46,373] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:46,374] INFO [Consumer clientId=connector-consumer-mongo-sink-19, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-19-d57944f4-3aed-4aee-839a-6eb147b41742 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:49,341] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,341] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,342] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,345] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,346] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,346] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,348] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,348] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,349] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,351] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,351] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,351] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,352] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,352] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,351] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,353] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,353] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,353] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,355] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,353] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,355] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,355] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,356] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,356] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:49,355] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,357] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,357] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,357] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,356] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,356] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:49,362] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Finished assignment for group at generation 11: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:49,368] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,368] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,369] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,369] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,369] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,369] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,369] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,370] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,371] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,371] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,371] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,372] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,372] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,372] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,372] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,372] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,374] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,373] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:49,373] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:49,372] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,376] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,376] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,372] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,377] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,377] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,375] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,377] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,374] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,378] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,378] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:49,379] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:49,380] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:49,374] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 11 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:49,381] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:49,382] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:49,388] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:49,385] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:49,383] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:49,397] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:49,399] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:49,422] ERROR WorkerSinkTask{id=mongo-sink-2} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:49,422] ERROR WorkerSinkTask{id=mongo-sink-2} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:49,422] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:49,423] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:49,424] INFO [Consumer clientId=connector-consumer-mongo-sink-2, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-2-5ff38764-59b2-4312-86fb-b008eb854006 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:52,370] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,370] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,370] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,373] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,373] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,373] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,376] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,376] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,376] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,377] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,377] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,377] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,380] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,380] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,381] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,381] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,381] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,381] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,381] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,382] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,382] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,384] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,384] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,384] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,387] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:52,388] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,388] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:52,393] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Finished assignment for group at generation 12: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:52,397] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,397] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,398] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,398] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,397] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,398] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,398] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,399] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,399] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,398] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,399] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,400] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,401] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,399] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,401] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:52,402] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:52,400] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,403] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,403] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,400] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:52,404] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,404] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,405] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,405] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,405] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,407] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:52,407] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,408] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,408] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:52,407] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:52,410] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:52,413] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:52,414] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:52,419] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:52,420] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:52,422] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:52,422] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:52,441] ERROR WorkerSinkTask{id=mongo-sink-3} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:52,442] ERROR WorkerSinkTask{id=mongo-sink-3} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:52,443] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:52,444] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:52,445] INFO [Consumer clientId=connector-consumer-mongo-sink-3, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-3-9bf0f150-be62-4b76-81ab-66a9e33e5e8e sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:53,728] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:30:53,728] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:30:53,732] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:30:55,400] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,401] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,401] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,402] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,402] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,402] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,401] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,408] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,408] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,409] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,410] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,410] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,410] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,410] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,410] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,411] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,411] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,412] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,412] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,413] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,413] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,429] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:55,429] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,429] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:55,432] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Finished assignment for group at generation 13: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:55,437] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,437] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,437] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,437] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,438] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,438] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,438] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,438] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,438] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,439] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,439] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,440] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,440] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,440] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,440] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,441] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,441] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,442] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:55,437] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,437] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,442] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:55,443] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:55,443] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,444] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,446] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:55,446] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:55,446] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:55,448] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:55,450] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:55,451] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:55,453] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:55,453] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:55,455] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:55,457] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:55,473] ERROR WorkerSinkTask{id=mongo-sink-4} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:55,479] ERROR WorkerSinkTask{id=mongo-sink-4} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:55,480] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:55,480] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:55,481] INFO [Consumer clientId=connector-consumer-mongo-sink-4, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-4-98ba9f4a-d920-4c50-abc2-7ba8149f4474 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:30:58,439] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:58,439] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,439] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:58,443] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:58,443] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,444] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:58,445] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:58,445] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,445] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:58,447] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:58,447] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,447] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:58,447] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:58,448] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,448] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:58,465] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:58,466] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,466] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:58,467] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:30:58,468] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,468] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:30:58,473] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Finished assignment for group at generation 14: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:30:58,475] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:58,476] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:58,476] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:58,476] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:58,476] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:58,477] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:58,477] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:58,477] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:58,478] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:58,478] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:58,478] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:58,478] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:58,479] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:58,479] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:58,480] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:58,480] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:58,480] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:58,481] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:58,483] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:58,477] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:58,483] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:58,486] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:58,483] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:58,478] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:30:58,489] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:30:58,489] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:30:58,491] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:58,492] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:58,491] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:30:58,497] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:58,508] ERROR WorkerSinkTask{id=mongo-sink-5} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:30:58,509] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:30:58,518] ERROR WorkerSinkTask{id=mongo-sink-5} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:30:58,518] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:30:58,519] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:30:58,520] INFO [Consumer clientId=connector-consumer-mongo-sink-5, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-5-c100752b-052a-43a6-bba8-fcbbaca31018 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:31:01,479] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:01,479] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:01,479] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:01,480] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:01,480] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:01,479] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:01,481] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:01,481] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:01,479] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:01,481] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:01,480] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:01,479] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:01,482] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:01,482] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:01,482] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:01,491] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:01,491] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:01,491] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:01,495] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Finished assignment for group at generation 15: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[test6-4])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:31:01,501] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:01,502] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:01,502] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:01,503] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:01,503] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:01,503] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:01,503] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:01,503] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:01,504] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:01,504] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:01,503] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:01,504] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:01,504] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:01,503] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:01,505] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:01,505] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:01,505] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:01,506] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:01,506] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:01,506] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:01,506] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:01,507] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:01,507] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:01,510] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:01,510] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:01,511] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:01,516] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:01,517] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:01,530] ERROR WorkerSinkTask{id=mongo-sink-7} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:31:01,531] ERROR WorkerSinkTask{id=mongo-sink-7} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:31:01,532] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:31:01,535] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:01,536] INFO [Consumer clientId=connector-consumer-mongo-sink-7, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-7-fc60a49f-ae80-40f7-854f-0ebd7def5f1f sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:31:03,732] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:31:03,733] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:31:03,739] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:31:04,504] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:04,504] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:04,504] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:04,504] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:04,504] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:04,505] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:04,505] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:04,504] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:04,506] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:04,505] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:04,506] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:04,504] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:04,508] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:04,509] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:04,509] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:04,512] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Finished assignment for group at generation 16: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-2]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-1]), connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c=Assignment(partitions=[test6-3])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:31:04,516] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 16 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:04,516] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 16 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:04,517] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Successfully joined group with generation 16 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:04,517] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:04,517] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:04,516] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 16 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:04,518] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:04,518] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:04,516] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 16 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:04,517] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:04,518] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:04,517] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:04,519] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:04,519] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:04,518] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:04,520] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:04,521] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:04,522] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:04,522] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:04,523] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:04,526] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:04,529] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:04,528] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:04,533] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:04,535] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:04,549] ERROR WorkerSinkTask{id=mongo-sink-8} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:31:04,553] ERROR WorkerSinkTask{id=mongo-sink-8} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:31:04,553] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:31:04,557] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:04,557] INFO [Consumer clientId=connector-consumer-mongo-sink-8, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-8-fc1615ae-bc08-4bef-ad04-7b65629f4e4c sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:31:07,517] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:07,517] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:07,517] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:07,519] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:07,519] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:07,519] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:07,521] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:07,521] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:07,521] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:07,522] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:07,522] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:07,522] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:07,526] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Finished assignment for group at generation 17: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0, test6-1]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007=Assignment(partitions=[test6-3]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-2])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:31:07,529] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 17 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:07,530] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 17 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:07,530] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:07,530] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:07,530] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Successfully joined group with generation 17 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:07,531] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:07,532] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:07,530] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 17 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:07,530] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0, test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:07,533] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:07,533] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:07,534] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:07,533] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:07,535] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:07,536] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:07,536] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:07,537] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:07,542] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:07,543] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:07,549] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:07,548] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:07,556] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:07,569] ERROR WorkerSinkTask{id=mongo-sink-10} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:31:07,570] ERROR WorkerSinkTask{id=mongo-sink-10} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:31:07,570] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:31:07,570] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:07,571] INFO [Consumer clientId=connector-consumer-mongo-sink-10, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-10-cf6c4948-6120-4bfa-aab4-417a619e1007 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:31:10,531] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:10,531] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:10,532] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:10,532] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:10,532] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:10,532] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:10,534] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:10,534] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:10,534] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:10,536] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Finished assignment for group at generation 18: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0, test6-1]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[test6-4]), connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782=Assignment(partitions=[test6-2, test6-3])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:31:10,539] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Successfully joined group with generation 18 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:10,540] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 18 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:10,539] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 18 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:10,540] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-2, test6-3]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:10,540] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:10,540] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:10,540] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0, test6-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:10,541] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:10,540] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3, test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:10,542] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:10,543] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:10,544] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:10,544] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:10,544] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:10,549] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:10,563] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:10,564] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:10,565] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:10,584] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:10,605] ERROR WorkerSinkTask{id=mongo-sink-1} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:31:10,610] ERROR WorkerSinkTask{id=mongo-sink-1} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:31:10,610] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:31:10,615] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3, test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:10,615] INFO [Consumer clientId=connector-consumer-mongo-sink-1, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-1-263382ae-ed20-4339-9d73-4cea12c6c782 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:31:13,542] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:13,543] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:13,543] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:13,544] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:13,545] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:13,545] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:13,548] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Finished assignment for group at generation 19: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0, test6-1, test6-2]), connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d=Assignment(partitions=[test6-3, test6-4])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:31:13,550] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Successfully joined group with generation 19 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:13,551] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-3, test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:13,551] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 19 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:13,551] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3, test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:13,552] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0, test6-1, test6-2]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:13,552] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-2, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:13,555] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:13,555] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:13,556] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:13,556] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:13,555] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:13,566] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:13,572] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:13,579] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:13,582] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:13,582] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:13,595] ERROR WorkerSinkTask{id=mongo-sink-9} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:31:13,596] ERROR WorkerSinkTask{id=mongo-sink-9} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:31:13,597] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:31:13,598] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3, test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:13,599] INFO [Consumer clientId=connector-consumer-mongo-sink-9, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-9-541445ec-d982-4d98-92fe-a3449096dd2d sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:31:13,746] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:31:13,746] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:31:13,752] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:31:16,555] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Attempt to heartbeat failed since group is rebalancing (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1082)
[2020-12-10 11:31:16,556] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-2, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:16,556] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:31:16,559] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Finished assignment for group at generation 20: {connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee=Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:31:16,563] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 20 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:31:16,564] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:31:16,566] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:31:16,567] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:16,568] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:16,568] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:16,568] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:16,568] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:31:16,599] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:16,599] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:16,599] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:16,600] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:16,603] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:31:16,609] ERROR WorkerSinkTask{id=mongo-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:31:16,613] ERROR WorkerSinkTask{id=mongo-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:31:16,613] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:31:16,613] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:31:16,614] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-0-2309cad0-daa2-40ea-aad7-ca05056eb2ee sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:31:23,754] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:31:23,754] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:31:23,759] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:31:33,760] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:31:33,761] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:31:33,766] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:31:43,768] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:31:43,768] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:31:43,773] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:31:53,775] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:31:53,775] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:31:53,778] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:32:03,779] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:32:03,780] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:32:03,786] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:32:13,787] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:32:13,787] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:32:13,796] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 9 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:32:23,797] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:32:23,797] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:32:23,804] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:32:33,806] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:32:33,806] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:32:33,811] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:32:43,811] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:32:43,811] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:32:43,816] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:32:53,817] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:32:53,817] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:32:53,822] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:33:03,823] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:33:03,823] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:33:03,827] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:33:13,829] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:33:13,829] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:33:13,834] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:33:23,834] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:33:23,834] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:33:23,839] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:33:33,840] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:33:33,840] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:33:33,844] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:33:43,845] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:33:43,845] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:33:43,851] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:33:53,853] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:33:53,853] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:33:53,858] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:34:03,859] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:34:03,861] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:34:03,966] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 106 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:34:13,969] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:34:13,969] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:34:13,973] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:34:23,973] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:34:23,974] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:34:23,977] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:34:33,981] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:34:33,981] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:34:33,985] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:34:43,985] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:34:43,986] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:34:43,990] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:34:53,992] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:34:53,993] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:34:54,001] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:35:04,002] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:35:04,002] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:35:04,007] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:35:14,008] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:35:14,008] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:35:14,011] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:35:24,012] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:35:24,012] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:35:24,016] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:35:34,017] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:35:34,017] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:35:34,023] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:35:44,024] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:35:44,024] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:35:44,028] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:35:54,029] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:35:54,029] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:35:54,034] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:36:04,035] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:36:04,035] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:36:04,039] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:36:14,040] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:36:14,040] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:36:14,044] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:36:24,046] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:36:24,046] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:36:24,050] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:36:34,051] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:36:34,051] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:36:34,055] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:36:44,056] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:36:44,056] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:36:44,060] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:36:54,061] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:36:54,061] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:36:54,065] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:37:04,066] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:37:04,066] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:37:04,070] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:37:14,071] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:37:14,071] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:37:14,078] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:37:24,079] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:37:24,079] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:37:24,082] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:37:34,083] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:37:34,083] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:37:34,089] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:37:44,090] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:37:44,091] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:37:44,097] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:37:54,098] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:37:54,099] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:37:54,102] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:37:55,313] ERROR Uncaught exception in REST call to //connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
javax.ws.rs.NotFoundException: HTTP 404 Not Found
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:250)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:760)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:547)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:221)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:173)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:500)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2020-12-10 11:37:55,512] ERROR Uncaught exception in REST call to /favicon.ico (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
javax.ws.rs.NotFoundException: HTTP 404 Not Found
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:250)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:760)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:547)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:221)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:173)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:500)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2020-12-10 11:38:04,103] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:38:04,103] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:38:04,108] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:38:14,109] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:38:14,110] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:38:14,115] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:38:24,116] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:38:24,116] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:38:24,119] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:38:34,120] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:38:34,120] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:38:34,124] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:38:44,125] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:38:44,126] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:38:44,130] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:38:54,131] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:38:54,131] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:38:54,137] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:39:04,138] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:39:04,138] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:39:04,143] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:39:14,144] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:39:14,144] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:39:14,151] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:39:24,151] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:39:24,152] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:39:24,156] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:39:34,156] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:39:34,156] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:39:34,160] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:39:44,161] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:39:44,162] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:39:44,165] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:39:54,165] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:39:54,165] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:39:54,170] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:40:04,171] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:40:04,172] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:40:04,175] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:40:14,176] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:40:14,177] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:40:14,182] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:40:24,183] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:40:24,183] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:40:24,187] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:40:34,188] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:40:34,189] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:40:34,193] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:40:44,195] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:40:44,195] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:40:44,199] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:40:54,201] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:40:54,201] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:40:54,207] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:41:04,209] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:41:04,209] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:41:04,212] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:41:14,214] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:41:14,214] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:41:14,217] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:41:24,218] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:41:24,218] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:41:24,221] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:41:34,223] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:41:34,223] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:41:34,227] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:41:44,228] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:41:44,228] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:41:44,232] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:41:54,233] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:41:54,233] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:41:54,237] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:41:56,943] INFO Successfully processed removal of connector 'mongo-sink' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2020-12-10 11:41:56,944] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector mongo-sink config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1513)
[2020-12-10 11:41:57,449] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling connector-only config update by stopping connector mongo-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:587)
[2020-12-10 11:41:57,449] INFO Stopping connector mongo-sink (org.apache.kafka.connect.runtime.Worker:387)
[2020-12-10 11:41:57,449] INFO Scheduled shutdown for WorkerConnector{id=mongo-sink} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2020-12-10 11:41:57,452] INFO Completed shutdown for WorkerConnector{id=mongo-sink} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2020-12-10 11:41:57,454] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:41:57,454] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:41:57,466] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 12 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:41:57,468] INFO Stopping connector mongo-sink (org.apache.kafka.connect.runtime.Worker:387)
[2020-12-10 11:41:57,468] INFO Stopping task mongo-sink-0 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,468] INFO Stopping task mongo-sink-1 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,468] INFO Stopping task mongo-sink-2 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,468] INFO Stopping task mongo-sink-3 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,468] WARN Ignoring stop request for unowned connector mongo-sink (org.apache.kafka.connect.runtime.Worker:390)
[2020-12-10 11:41:57,469] WARN Ignoring await stop request for non-present connector mongo-sink (org.apache.kafka.connect.runtime.Worker:415)
[2020-12-10 11:41:57,469] INFO Stopping task mongo-sink-4 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,470] INFO Stopping task mongo-sink-5 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,470] INFO Stopping task mongo-sink-6 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,471] INFO Stopping task mongo-sink-7 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,478] INFO Stopping task mongo-sink-8 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,478] INFO Stopping task mongo-sink-9 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,480] INFO Stopping task mongo-sink-10 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,482] INFO Stopping task mongo-sink-11 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,483] INFO Stopping task mongo-sink-12 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,486] INFO Stopping task mongo-sink-14 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,486] INFO Stopping task mongo-sink-15 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,487] INFO Stopping task mongo-sink-16 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,486] INFO Stopping task mongo-sink-13 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,492] INFO Stopping task mongo-sink-18 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,491] INFO Stopping task mongo-sink-17 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,493] INFO Stopping task mongo-sink-19 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:41:57,498] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1759)
[2020-12-10 11:41:57,509] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1778)
[2020-12-10 11:41:57,509] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 12 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=37, connectorIds=[mqtt-source], taskIds=[mqtt-source-0], revokedConnectorIds=[mongo-sink], revokedTaskIds=[mongo-sink-0, mongo-sink-1, mongo-sink-2, mongo-sink-3, mongo-sink-4, mongo-sink-5, mongo-sink-6, mongo-sink-7, mongo-sink-8, mongo-sink-9, mongo-sink-10, mongo-sink-11, mongo-sink-12, mongo-sink-13, mongo-sink-14, mongo-sink-15, mongo-sink-16, mongo-sink-17, mongo-sink-18, mongo-sink-19], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:41:57,555] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 37 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:41:57,556] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:41:57,557] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:41:57,557] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:41:57,575] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 13 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:41:57,576] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 13 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=37, connectorIds=[mqtt-source], taskIds=[mqtt-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:41:57,576] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 37 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:41:57,578] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:42:04,238] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:42:04,239] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:42:04,244] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:42:14,246] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:42:14,246] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:42:14,298] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 52 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:42:24,299] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:42:24,299] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:42:24,303] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:42:34,304] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:42:34,304] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:42:34,307] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:42:44,309] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:42:44,310] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:42:44,351] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 41 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:42:54,352] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:42:54,352] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:42:54,358] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:43:04,359] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:43:04,359] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:43:04,364] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:43:05,265] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:43:05,268] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:43:05,268] INFO Cluster created with settings {hosts=[18.140.231.254:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500} (org.mongodb.driver.cluster:71)
[2020-12-10 11:43:05,649] INFO Opened connection [connectionId{localValue:3, serverValue:184985}] to 18.140.231.254:27017 (org.mongodb.driver.connection:71)
[2020-12-10 11:43:05,779] INFO Monitor thread successfully connected to server with description ServerDescription{address=18.140.231.254:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, version=ServerVersion{versionList=[4, 4, 1]}, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=130239317, setName='app1r1', canonicalAddress=ip-172-31-1-240:27017, hosts=[ip-172-31-1-240:27017], passives=[], arbiters=[], primary='ip-172-31-1-240:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, lastWriteDate=Thu Dec 10 11:43:08 EST 2020, lastUpdateTimeNanos=34896618658833} (org.mongodb.driver.cluster:71)
[2020-12-10 11:43:06,607] INFO Opened connection [connectionId{localValue:4, serverValue:184986}] to 18.140.231.254:27017 (org.mongodb.driver.connection:71)
[2020-12-10 11:43:06,883] INFO Closed connection [connectionId{localValue:4, serverValue:184986}] to 18.140.231.254:27017 because the pool has been closed. (org.mongodb.driver.connection:71)
[2020-12-10 11:43:06,884] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:354)
[2020-12-10 11:43:06,889] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector mongo-sink config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1526)
[2020-12-10 11:43:07,394] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:43:07,395] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:43:07,401] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 14 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:43:07,401] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 14 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=38, connectorIds=[mongo-sink, mqtt-source], taskIds=[mqtt-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:43:07,402] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 38 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:43:07,402] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector mongo-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1291)
[2020-12-10 11:43:07,402] INFO Creating connector mongo-sink of type com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2020-12-10 11:43:07,402] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:43:07,403] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:43:07,404] INFO Instantiated connector mongo-sink with version 1.3.0 of type class com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2020-12-10 11:43:07,409] INFO Finished creating connector mongo-sink (org.apache.kafka.connect.runtime.Worker:310)
[2020-12-10 11:43:07,410] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:43:07,413] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:43:07,414] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:43:08,413] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [mongo-sink-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1541)
[2020-12-10 11:43:08,917] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:641)
[2020-12-10 11:43:08,917] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:43:08,917] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:43:08,922] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 15 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:43:08,923] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 15 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=40, connectorIds=[mongo-sink, mqtt-source], taskIds=[mongo-sink-0, mqtt-source-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:43:08,923] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 40 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:43:08,923] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:43:08,925] INFO Creating task mongo-sink-0 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:43:08,925] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:43:08,926] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:43:08,927] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:43:08,927] INFO Instantiated task mongo-sink-0 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:43:08,927] INFO StringConverterConfig values: 
	converter.encoding = UTF8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:354)
[2020-12-10 11:43:08,928] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:43:08,928] INFO Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongo-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:43:08,928] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:43:08,928] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:43:08,929] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:43:08,929] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:43:08,930] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:43:08,930] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:43:08,932] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:43:08,932] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:43:08,933] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:43:08,933] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:43:08,933] INFO Kafka startTimeMs: 1607618588933 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:43:08,939] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:43:08,946] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:43:08,947] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:43:08,948] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:43:08,949] INFO WorkerSinkTask{id=mongo-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:43:08,950] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:43:08,950] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:43:09,052] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:43:09,052] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:43:09,154] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Connection to node -1 (localhost/127.0.0.1:9090) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:757)
[2020-12-10 11:43:09,154] WARN [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Bootstrap broker localhost:9090 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1033)
[2020-12-10 11:43:09,260] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:43:09,264] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:43:09,265] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:43:09,271] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:43:09,271] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:43:09,280] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Finished assignment for group at generation 1: {connector-consumer-mongo-sink-0-15d948c0-c24f-4961-b984-9e76905bcb22=Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:43:09,283] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:43:09,284] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:43:09,284] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:43:09,287] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:43:09,287] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:43:09,288] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:43:09,288] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:43:09,288] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:43:09,310] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:43:09,313] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:43:09,314] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:43:09,318] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:43:09,319] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:43:09,330] ERROR WorkerSinkTask{id=mongo-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.TransformationChain.apply(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:519)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Only Map objects supported in absence of schema for [copying fields from value to key], found: java.lang.String
	at org.apache.kafka.connect.transforms.util.Requirements.requireMap(Requirements.java:38)
	at org.apache.kafka.connect.transforms.ValueToKey.applySchemaless(ValueToKey.java:72)
	at org.apache.kafka.connect.transforms.ValueToKey.apply(ValueToKey.java:65)
	at org.apache.kafka.connect.runtime.TransformationChain.lambda$apply$0(TransformationChain.java:50)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 14 more
[2020-12-10 11:43:09,331] ERROR WorkerSinkTask{id=mongo-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:43:09,331] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:43:09,338] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:43:09,338] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-0-15d948c0-c24f-4961-b984-9e76905bcb22 sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
[2020-12-10 11:43:14,365] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:43:14,365] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:43:14,372] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:43:24,373] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:43:24,374] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:43:24,379] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:43:34,380] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:43:34,380] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:43:34,386] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:43:44,386] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:43:44,387] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:43:44,390] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:43:54,391] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:43:54,391] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:43:54,395] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:44:04,395] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:44:04,396] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:44:04,402] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 6 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:44:14,403] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:44:14,403] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:44:14,406] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:44:24,407] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:44:24,408] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:44:24,412] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:44:34,413] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:44:34,413] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:44:34,418] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:44:44,419] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:44:44,420] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:44:44,424] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:44:54,425] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:44:54,425] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:44:54,429] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:45:04,430] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:45:04,431] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:45:04,438] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:45:14,440] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:45:14,440] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:45:14,444] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:45:24,445] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:45:24,445] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:45:24,449] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:45:34,450] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:45:34,450] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:45:34,453] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:45:44,454] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:45:44,454] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:45:44,459] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:45:54,460] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:45:54,460] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:45:54,470] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 10 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:46:04,471] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:46:04,472] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:46:04,512] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 41 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:46:14,513] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:46:14,513] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:46:14,517] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 4 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:46:24,518] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:46:24,518] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:46:24,521] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 3 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:46:34,523] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:46:34,523] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:46:34,528] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:46:44,528] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:46:44,528] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:46:44,533] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:46:54,534] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:46:54,534] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:46:54,539] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:47:02,238] INFO Successfully processed removal of connector 'mqtt-source' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2020-12-10 11:47:02,239] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector mqtt-source config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1513)
[2020-12-10 11:47:02,742] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling connector-only config update by stopping connector mqtt-source (org.apache.kafka.connect.runtime.distributed.DistributedHerder:587)
[2020-12-10 11:47:02,743] INFO Stopping connector mqtt-source (org.apache.kafka.connect.runtime.Worker:387)
[2020-12-10 11:47:02,744] INFO Scheduled shutdown for WorkerConnector{id=mqtt-source} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2020-12-10 11:47:02,745] INFO Completed shutdown for WorkerConnector{id=mqtt-source} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2020-12-10 11:47:02,746] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:47:02,746] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:47:02,756] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 16 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:47:02,757] INFO Stopping connector mqtt-source (org.apache.kafka.connect.runtime.Worker:387)
[2020-12-10 11:47:02,757] WARN Ignoring stop request for unowned connector mqtt-source (org.apache.kafka.connect.runtime.Worker:390)
[2020-12-10 11:47:02,757] WARN Ignoring await stop request for non-present connector mqtt-source (org.apache.kafka.connect.runtime.Worker:415)
[2020-12-10 11:47:02,757] INFO Stopping task mqtt-source-0 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:47:02,778] INFO WorkerSourceTask{id=mqtt-source-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:478)
[2020-12-10 11:47:02,779] INFO WorkerSourceTask{id=mqtt-source-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:495)
[2020-12-10 11:47:02,784] INFO WorkerSourceTask{id=mqtt-source-0} Finished commitOffsets successfully in 5 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:574)
[2020-12-10 11:47:02,784] INFO [Producer clientId=connector-producer-mqtt-source-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1189)
[2020-12-10 11:47:02,790] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1759)
[2020-12-10 11:47:02,800] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1778)
[2020-12-10 11:47:02,801] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 16 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=42, connectorIds=[mongo-sink], taskIds=[mongo-sink-0], revokedConnectorIds=[mqtt-source], revokedTaskIds=[mqtt-source-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:47:02,801] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 42 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:47:02,801] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:47:02,801] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:47:02,801] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:47:02,808] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 17 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:47:02,809] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 17 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=42, connectorIds=[mongo-sink], taskIds=[mongo-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:47:02,809] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 42 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:47:02,809] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:47:14,120] INFO Successfully processed removal of connector 'mongo-sink' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:578)
[2020-12-10 11:47:14,120] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector mongo-sink config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1513)
[2020-12-10 11:47:14,625] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling connector-only config update by stopping connector mongo-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:587)
[2020-12-10 11:47:14,625] INFO Stopping connector mongo-sink (org.apache.kafka.connect.runtime.Worker:387)
[2020-12-10 11:47:14,625] INFO Scheduled shutdown for WorkerConnector{id=mongo-sink} (org.apache.kafka.connect.runtime.WorkerConnector:249)
[2020-12-10 11:47:14,626] INFO Completed shutdown for WorkerConnector{id=mongo-sink} (org.apache.kafka.connect.runtime.WorkerConnector:269)
[2020-12-10 11:47:14,626] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:47:14,626] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:47:14,635] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 18 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:47:14,635] INFO Stopping task mongo-sink-0 (org.apache.kafka.connect.runtime.Worker:836)
[2020-12-10 11:47:14,635] INFO Stopping connector mongo-sink (org.apache.kafka.connect.runtime.Worker:387)
[2020-12-10 11:47:14,636] WARN Ignoring stop request for unowned connector mongo-sink (org.apache.kafka.connect.runtime.Worker:390)
[2020-12-10 11:47:14,636] WARN Ignoring await stop request for non-present connector mongo-sink (org.apache.kafka.connect.runtime.Worker:415)
[2020-12-10 11:47:14,636] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1759)
[2020-12-10 11:47:14,640] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1778)
[2020-12-10 11:47:14,641] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 18 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=44, connectorIds=[], taskIds=[], revokedConnectorIds=[mongo-sink], revokedTaskIds=[mongo-sink-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:47:14,641] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 44 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:47:14,642] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:47:14,642] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:47:14,642] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:47:14,656] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 19 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:47:14,657] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 19 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=44, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:47:14,657] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 44 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:47:14,657] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:47:49,198] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:61)
com.fasterxml.jackson.databind.JsonMappingException: Unexpected character ('"' (code 34)): was expecting comma to separate Object entries
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 21, column: 6] (through reference chain: org.apache.kafka.connect.runtime.rest.entities.CreateConnectorRequest["config"])
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:397)
	at com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:356)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.wrapAndThrow(BeanDeserializerBase.java:1719)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:530)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:417)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1292)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:159)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:1682)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:977)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:814)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:877)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:274)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:366)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:319)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:760)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:547)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1607)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1297)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:485)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1577)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1212)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:221)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:173)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:500)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:383)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:547)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:375)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:270)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
	at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)
	at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)
	at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('"' (code 34)): was expecting comma to separate Object entries
 at [Source: (org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$UnCloseableInputStream); line: 21, column: 6]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1840)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:712)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:637)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1011)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:512)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:364)
	at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:29)
	at com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:530)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:528)
	... 70 more
[2020-12-10 11:48:14,661] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:48:14,670] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:48:14,672] INFO Cluster created with settings {hosts=[18.140.231.254:27017], mode=SINGLE, requiredClusterType=UNKNOWN, serverSelectionTimeout='30000 ms', maxWaitQueueSize=500} (org.mongodb.driver.cluster:71)
[2020-12-10 11:48:15,053] INFO Opened connection [connectionId{localValue:5, serverValue:184987}] to 18.140.231.254:27017 (org.mongodb.driver.connection:71)
[2020-12-10 11:48:15,172] INFO Monitor thread successfully connected to server with description ServerDescription{address=18.140.231.254:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, version=ServerVersion{versionList=[4, 4, 1]}, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=118975767, setName='app1r1', canonicalAddress=ip-172-31-1-240:27017, hosts=[ip-172-31-1-240:27017], passives=[], arbiters=[], primary='ip-172-31-1-240:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000001, setVersion=1, lastWriteDate=Thu Dec 10 11:48:17 EST 2020, lastUpdateTimeNanos=35206011349365} (org.mongodb.driver.cluster:71)
[2020-12-10 11:48:15,996] INFO Opened connection [connectionId{localValue:6, serverValue:184988}] to 18.140.231.254:27017 (org.mongodb.driver.connection:71)
[2020-12-10 11:48:16,266] INFO Closed connection [connectionId{localValue:6, serverValue:184988}] to 18.140.231.254:27017 because the pool has been closed. (org.mongodb.driver.connection:71)
[2020-12-10 11:48:16,268] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:354)
[2020-12-10 11:48:16,275] INFO [Worker clientId=connect-1, groupId=connect-cluster] Connector mongo-sink config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1526)
[2020-12-10 11:48:16,776] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:48:16,777] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:48:16,782] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 20 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:48:16,782] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 20 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=45, connectorIds=[mongo-sink], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:48:16,783] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 45 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:48:16,783] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connector mongo-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1291)
[2020-12-10 11:48:16,783] INFO Creating connector mongo-sink of type com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:274)
[2020-12-10 11:48:16,783] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:48:16,784] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:48:16,784] INFO Instantiated connector mongo-sink with version 1.3.0 of type class com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:284)
[2020-12-10 11:48:16,786] INFO Finished creating connector mongo-sink (org.apache.kafka.connect.runtime.Worker:310)
[2020-12-10 11:48:16,786] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:48:16,787] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:48:16,788] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:48:17,794] INFO [Worker clientId=connect-1, groupId=connect-cluster] Tasks [mongo-sink-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1541)
[2020-12-10 11:48:17,795] INFO [Worker clientId=connect-1, groupId=connect-cluster] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder:641)
[2020-12-10 11:48:17,795] INFO [Worker clientId=connect-1, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:225)
[2020-12-10 11:48:17,795] INFO [Worker clientId=connect-1, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:48:17,798] INFO [Worker clientId=connect-1, groupId=connect-cluster] Successfully joined group with generation 21 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:48:17,798] INFO [Worker clientId=connect-1, groupId=connect-cluster] Joined group at generation 21 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-31e49eff-4021-4460-91ae-26b6c1250692', leaderUrl='http://127.0.1.1:8083/', offset=47, connectorIds=[mongo-sink], taskIds=[mongo-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1681)
[2020-12-10 11:48:17,798] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting connectors and tasks using config offset 47 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1208)
[2020-12-10 11:48:17,799] INFO [Worker clientId=connect-1, groupId=connect-cluster] Starting task mongo-sink-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1250)
[2020-12-10 11:48:17,799] INFO Creating task mongo-sink-0 (org.apache.kafka.connect.runtime.Worker:509)
[2020-12-10 11:48:17,799] INFO ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:354)
[2020-12-10 11:48:17,800] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:48:17,800] INFO TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:354)
[2020-12-10 11:48:17,800] INFO Instantiated task mongo-sink-0 with version 1.3.0 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:524)
[2020-12-10 11:48:17,800] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:48:17,800] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:354)
[2020-12-10 11:48:17,801] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:539)
[2020-12-10 11:48:17,801] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongo-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:545)
[2020-12-10 11:48:17,801] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongo-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:550)
[2020-12-10 11:48:17,803] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey} (org.apache.kafka.connect.runtime.Worker:632)
[2020-12-10 11:48:17,803] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:354)
[2020-12-10 11:48:17,804] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongo-sink
	predicates = []
	tasks.max = 1
	topics = [test6]
	topics.regex = 
	transforms = [createKey]
	transforms.createKey.fields = [deviceId]
	transforms.createKey.negate = false
	transforms.createKey.predicate = 
	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:354)
[2020-12-10 11:48:17,805] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9090, localhost:9092, localhost:9093, localhost:9094]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongo-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongo-sink
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:354)
[2020-12-10 11:48:17,823] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:48:17,823] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig:362)
[2020-12-10 11:48:17,823] INFO Kafka version: 6.0.0-ccs (org.apache.kafka.common.utils.AppInfoParser:117)
[2020-12-10 11:48:17,823] INFO Kafka commitId: 17b744c31e00868b (org.apache.kafka.common.utils.AppInfoParser:118)
[2020-12-10 11:48:17,823] INFO Kafka startTimeMs: 1607618897823 (org.apache.kafka.common.utils.AppInfoParser:119)
[2020-12-10 11:48:17,826] INFO [Worker clientId=connect-1, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
[2020-12-10 11:48:17,831] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Subscribed to topic(s): test6 (org.apache.kafka.clients.consumer.KafkaConsumer:965)
[2020-12-10 11:48:17,832] INFO Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:87)
[2020-12-10 11:48:17,834] INFO MongoSinkTopicConfig values: 
	change.data.capture.handler = 
	collection = kcol
	database = abmVentilator
	delete.on.null.values = false
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.PartialValueStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = false
	errors.tolerance = none
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	max.num.retries = 1
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	retries.defer.timeout = 5000
	topic = test6
	value.projection.list = deviceId
	value.projection.type = allowlist
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneBusinessKeyStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:354)
[2020-12-10 11:48:17,840] INFO WorkerSinkTask{id=mongo-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:303)
[2020-12-10 11:48:17,843] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Cluster ID: k63hXRb0SeW0on3FPsxu1w (org.apache.kafka.clients.Metadata:279)
[2020-12-10 11:48:17,848] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Discovered group coordinator kali:9093 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:815)
[2020-12-10 11:48:17,850] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:48:17,855] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Join group failed with org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:456)
[2020-12-10 11:48:17,855] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:553)
[2020-12-10 11:48:17,859] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Finished assignment for group at generation 1: {connector-consumer-mongo-sink-0-131bf1de-4779-43ac-8692-905beaedbaae=Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:627)
[2020-12-10 11:48:17,862] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:504)
[2020-12-10 11:48:17,863] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Notifying assignor about the new Assignment(partitions=[test6-0, test6-1, test6-2, test6-3, test6-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:276)
[2020-12-10 11:48:17,863] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Adding newly assigned partitions: test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:288)
[2020-12-10 11:48:17,864] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:48:17,864] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:48:17,865] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:48:17,865] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:48:17,865] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Found no committed offset for partition test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1349)
[2020-12-10 11:48:17,876] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:48:17,876] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:48:17,876] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:48:17,877] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:48:17,878] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Resetting offset for partition test6-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:397)
[2020-12-10 11:48:17,895] ERROR WorkerSinkTask{id=mongo-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:196)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:492)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error: 
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:366)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$0(WorkerSinkTask.java:492)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	... 13 more
Caused by: org.apache.kafka.common.errors.SerializationException: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'ION': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (byte[])"ION/VM005/CURRENTALARM"; line: 1, column: 5]
Caused by: com.fasterxml.jackson.core.JsonParseException: Unrecognized token 'ION': was expecting (JSON String, Number, Array, Object or token 'null', 'true' or 'false')
 at [Source: (byte[])"ION/VM005/CURRENTALARM"; line: 1, column: 5]
	at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1840)
	at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:722)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3560)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._reportInvalidToken(UTF8StreamJsonParser.java:3536)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._matchToken(UTF8StreamJsonParser.java:2854)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._handleUnexpectedValue(UTF8StreamJsonParser.java:2639)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser._nextTokenNotInObject(UTF8StreamJsonParser.java:857)
	at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:754)
	at com.fasterxml.jackson.databind.ObjectMapper._readTreeAndClose(ObjectMapper.java:4247)
	at com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:2734)
	at org.apache.kafka.connect.json.JsonDeserializer.deserialize(JsonDeserializer.java:64)
	at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:364)
	at org.apache.kafka.connect.storage.Converter.toConnectData(Converter.java:87)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.lambda$convertAndTransformRecord$0(WorkerSinkTask.java:492)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndRetry(RetryWithToleranceOperator.java:146)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:180)
	at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute(RetryWithToleranceOperator.java:122)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertAndTransformRecord(WorkerSinkTask.java:492)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.convertMessages(WorkerSinkTask.java:472)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:322)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
[2020-12-10 11:48:17,898] ERROR WorkerSinkTask{id=mongo-sink-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
[2020-12-10 11:48:17,898] INFO Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:175)
[2020-12-10 11:48:17,899] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Revoke previously assigned partitions test6-3, test6-2, test6-4, test6-1, test6-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:307)
[2020-12-10 11:48:17,900] INFO [Consumer clientId=connector-consumer-mongo-sink-0, groupId=connect-mongo-sink] Member connector-consumer-mongo-sink-0-131bf1de-4779-43ac-8692-905beaedbaae sending LeaveGroup request to coordinator kali:9093 (id: 2147483646 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:1005)
